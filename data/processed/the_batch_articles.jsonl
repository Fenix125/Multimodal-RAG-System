{"article_id":"opus-4-5-drops-prices-reclaims-coding-crown","title":"Opus 4.5 drops prices, reclaims coding crown: DeepSeek’s new open-weights math model claims IMO Gold","url":"https://www.deeplearning.ai/the-batch/opus-4-5-drops-prices-reclaims-coding-crown/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-28T09:22:00-08:00","body_text":"Welcome back! In today’s edition of Data Points, you’ll learn more about: FLUX.2, a Nano Banana-class open image generator OpenAI’s third-party web analytics security breach Genesis Mission, a new U.S. government resource sharing program Suno’s latest deal with music giant Warner But first: Claude Opus 4.5 achieves SOTA coding and agent capabilities Anthropic launched Claude Opus 4.5, positioning it as the leading model for software engineering, autonomous agents, and computer use. The model leads on seven out of eight programming languages on SWE-bench Multilingual and scored higher than any human candidate on Anthropic’s internal performance engineering exam within a two-hour time limit. The release includes a new effort parameter that lets developers control token usage; at medium effort, Opus 4.5 matches Sonnet 4.5’s performance while using 76 percent fewer output tokens. Opus 4.5 is available now through Claude apps, API, and major cloud platforms at five dollars per million input tokens and 25 dollars per million output tokens. ( Anthropic ) DeepSeekMath-V2 claims International Mathematics Olympiad Gold DeepSeek introduced DeepSeekMath-V2, a language model that verifies the correctness of its own mathematical reasoning step-by-step rather than relying solely on final answer accuracy. The model achieved gold-level scores on IMO 2025 and CMO 2024 competitions and scored 118 out of 120 on Putnam 2024 when using scaled test-time compute. The system trains a verifier to check theorem proofs for comprehensiveness and rigor, then uses that verifier as a reward model to train a proof generator that identifies and fixes issues in its own work before finalizing solutions. The approach addresses limitations in current reinforcement learning methods that reward correct final answers but can’t verify reasoning quality or handle tasks like theorem proving, which require rigorous derivation without numerical answers. The model’s weights are available freely on GitHub. ( GitHub ) Black Forest Labs launches FLUX.2 with multi-reference editing Black Forest Labs’ image generation model FLUX.2 handles up to 10 reference images simultaneously while maintaining character and style consistency, and edits images at resolutions up to 4 megapixels. The model combines a Mistral-3 24-billion-parameter vision-language model with a rectified flow transformer, improving text rendering, prompt adherence, and photorealism over FLUX.1. The company released four variants: FLUX.2 [pro] and [flex] as managed APIs, FLUX.2 [dev] as a 32-billion-parameter open-weight model available on Hugging Face under a non-commercial license, and FLUX.2 [klein] coming soon as an Apache 2.0 licensed model. The FLUX.2 VAE is available now under Apache 2.0 license, with API access through partners including FAL, Replicate, Runware, TogetherAI, Cloudflare, and DeepInfra. ( Black Forest Labs ) OpenAI discloses security incident at external analytics provider A breach at Mixpanel, a web analytics service OpenAI used for its API platform, exposed user profile data for some API customers. The November 9 incident affected names, email addresses, coarse location data, browser information, and organization IDs associated with platform.openai.com accounts. No chat content, API requests, passwords, API keys, payment information, or government IDs were compromised. OpenAI terminated its contract with Mixpanel and is conducting expanded security reviews across its vendor ecosystem. The company recommends users stay alert for phishing attempts using the exposed profile information and enable multi-factor authentication as a precaution. ( OpenAI ) White House “Genesis” to lend government data to AI companies U.S. President Donald Trump signed an executive order directing the Department of Energy and national labs to build a digital platform consolidating federal scientific data for AI analysis. The Genesis Mission solicits tech companies and universities to apply their AI systems to government challenges in engineering, energy, and national security, including optimizing the electric grid. The administration compared the effort to the Apollo program, though it follows billions in cuts to federal research funding and thousands of job losses among government scientists. Funding comes from the tax and spending bill Trump signed in July, with the project using both national lab supercomputers and private sector computing capacity. ( Associated Press ) Suno partners with Warner to train AI models on licensed catalog Suno signed a deal with Warner Music Group to build new AI music generation models trained on WMG’s licensed recordings. The partnership will introduce features letting fans create music using the voices and likenesses of participating WMG artists, with those artists receiving compensation. Suno will require a paid subscription to download generated songs, though its Studio product will retain unlimited downloads. The company said its core music creation experience will remain unchanged while it develops what it calls a new generation of models that will surpass its current v5 system. Warner Music and Suno also settled their copyright lawsuit; similar cases against Suno by Universal and Sony continue. ( Suno ) A special offer for our community DeepLearning.AI recently launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng talked about the potential AI bubble, highlighting underinvestment in AI applications, the need for more AI infrastructure for inference, and the risks associated with AI infrastructure for model training. “I remain bullish about AI investments broadly. But what is the downside scenario — that is, is there a bubble that will pop? One scenario that worries me: If part of the AI stack (perhaps in training infra) suffers from overinvestment and collapses, it could lead to negative market sentiment around AI more broadly and an irrational outflow of interest away from investing in AI, despite the field overall having strong fundamentals.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: Google led arena leaderboards with Gemini 3 Pro and Nano Banana Pro, showcasing best-in-class multimodal reasoning and image generation capabilities. Microsoft and Anthropic formed an alliance , making Claude the first leading language model available from all three cloud giants. Record labels backed AI-music startup Klay Image, which secured deals with industry giants Sony, Warner, and Universal. Researchers introduced Persona Vectors to help model builders identify and edit out sycophancy, hallucinations, and more. Subscribe to Data Points","images":[{"image_id":"opus-4-5-drops-prices-reclaims-coding-crown_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/DeepSeekV2Mathematicians--Take-2-.png","local_path":null,"alt":"Classroom setting with mathematicians working on math equations and examining data graphs."}]}
{"article_id":"a-robot-holds-a-bubble-wand-surrounded-by-bubbles-and-colorful-trees-with-a-futuristic-city-skyline","title":"Understanding the AI Bubble — If There Is One","url":"https://www.deeplearning.ai/the-batch/a-robot-holds-a-bubble-wand-surrounded-by-bubbles-and-colorful-trees-with-a-futuristic-city-skyline/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Nov 26, 2025"],"published_at":"2025-11-26T13:23:50-08:00","body_text":"Dear friends, Is there an AI bubble? With the massive number of dollars going into AI infrastructure such as OpenAI’s $1.4 trillion plan and Nvidia briefly reaching a $5 trillion market cap, many have asked if speculation and hype have driven the values of AI investments above sustainable values. However, AI isn’t monolithic, and different areas look bubbly to different degrees. AI application layer: There is underinvestment. The potential is still much greater than most realize. AI infrastructure for inference: This still needs significant investment. AI infrastructure for model training: I’m still cautiously optimistic about this sector, but there could also be a bubble. Caveat: I am absolutely not giving investment advice! AI application layer. There are many applications yet to be built over the coming decade using new AI technology. Almost by definition, applications that are built on top of AI infrastructure/technology (such as LLM APIs) have to be more valuable than the infrastructure, since we need them to be able to pay the infrastructure and technology providers. I am seeing many green shoots across many businesses that are applying agentic workflows, and am confident this will grow! I have also spoken with many Venture Capital investors who hesitate to invest in AI applications because they feel they don’t know how to pick winners, whereas the recipe for deploying $1B to build AI infrastructure is better understood. Some have also bought into the hype that almost all AI applications will be wiped out merely by frontier LLM companies improving their foundation models. Overall, I believe there is significant underinvestment in AI applications. This area remains a huge focus for my venture studio, AI Fund. AI infrastructure for inference. Despite AI’s low penetration today, infrastructure providers are already struggling to fulfill demand for processing power to generate tokens. Several of my teams are worried about whether we can get enough inference capacity, and both cost and inference throughput are limiting our ability to use even more. It is a good problem to have that businesses are supply-constrained rather than demand-constrained. The latter is a much more common problem, when not enough people want your product. But insufficient supply is nonetheless a problem, which is why I am glad our industry is investing significantly in scaling up inference capacity. As one concrete example of high demand for token generation, highly agentic coders are progressing rapidly. I’ve long been a fan of Claude Code; OpenAI Codex also improved dramatically with the release of GPT-5; and Gemini 3 has made Google CLI very competitive. As these tools improve, their adoption will grow. At the same time, overall market penetration is still low, and many developers are still using older generations of coding tools (and some aren’t even using any agentic coding tools). As market penetration grows —  I’m confident it will, given how useful these tools are — aggregate demand for token generation will grow. I predicted early last year that we’d need more inference capacity, partly because of agentic workflows. Since then, the need has become more acute. As a society, we need more capacity for AI inference! Having said that, I’m not saying it’s impossible to lose money investing in this sector. If we end up overbuilding — and I don’t currently know if we will — then providers may end up having to sell capacity at a loss or at low returns. I hope investors in this space do well financially. The good news, however, is that even if we overbuild, this capacity will get used, and it will be good for application builders! AI infrastructure for model training. I am happy to see the investments going into training bigger models. But, of the three buckets of investments, this seems the riskiest. If open -source/open-weight models continue to grow in market share, then some companies that are pouring billions into training models might not see an attractive financial return on their investment. Additionally, algorithmic and hardware improvements are making it cheaper each year to train models of a given level of capability, so the “technology moat” for training frontier models is  weak. (That said, ChatGPT has become a strong consumer brand, and so it enjoys a strong brand moat, while Gemini, assisted by Google's massive distribution advantage, is also making a strong showing.) I remain bullish about AI investments broadly. But what is the downside scenario — that is, is there a bubble that will pop? One scenario that worries me: If part of the AI stack (perhaps in training infra) suffers from overinvestment and collapses, it could lead to negative market sentiment around AI more broadly and an irrational outflow of interest away from investing in AI, despite the field overall having strong fundamentals. I don’t think this will happen, but if it does, it would be unfortunate since there’s still a lot of work in AI that I consider highly deserving of much more investment. Warren Buffett popularized Benjamin Graham’s quote, “In the short run, the market is a voting machine, but in the long run, it is a weighing machine.” He meant that in the short term, stock prices are driven by investor sentiment and speculation; but in the long term, they are driven by fundamental, intrinsic value. I find it hard to forecast sentiment and speculation, but am very confident about the long-term health of AI’s fundamentals. So my plan is just to keep building! Andrew","images":[{"image_id":"a-robot-holds-a-bubble-wand-surrounded-by-bubbles-and-colorful-trees-with-a-futuristic-city-skyline_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Understanding-the-AI-Bubble---If-There-Is-One-2.png","local_path":null,"alt":"A robot holds a bubble wand, surrounded by bubbles and colorful trees, with a futuristic city skyline."}]}
{"article_id":"inside-olmo-3-a-new-family-of-fully-open-models","title":"Inside Olmo 3, a new family of fully open models: Grok 4.1’s uneasy balance between EQ and sycophancy","url":"https://www.deeplearning.ai/the-batch/inside-olmo-3-a-new-family-of-fully-open-models/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-24T11:47:00-08:00","body_text":"Welcome back! In today’s edition of Data Points, you’ll learn more about: Nano Banana Pro, Google’s updated image generator Anthropic’s latest partnerships with Microsoft and Nvidia Memo, a home robot trained on real-life human tasks A new AI play modeled on legendary French playwright Molière’s work But first: Olmo 3 opens complete development pipeline to researchers The Allen Institute for AI released Olmo 3, a family of open-source language models that exposes the entire “model flow”: every training stage, checkpoint, dataset, and dependency required to create and modify the models. The release includes Olmo 3-Base (7 billion and 32 billion parameters), Olmo 3-Think (the strongest fully open 32 billion-parameter reasoning model), Olmo 3-Instruct (for chat and tool use), and Olmo 3-RL Zero (for reinforcement learning experiments). Olmo 3-Base outperforms other fully open base models on benchmarks for programming, reading comprehension, and math, while Olmo 3-Think narrows the gap with leading open-weight models like Qwen 3 despite training on roughly six times fewer tokens. The release enables researchers to trace model behaviors back to specific training data and decisions, fork development at any stage, and conduct experiments that require full visibility into how AI systems learn, all of which help address concerns about transparency and accountability in AI development. All components, including the 9.3 trillion-token Dolma 3 training corpus and post-training datasets, are available under permissive open-source licenses. ( Allen AI ) xAI’s Grok 4.1 tops emotional intelligence leaderboard Grok 4.1 now leads EQ-Bench3, a benchmark that measures how well language models handle emotional intelligence through roleplay scenarios. The model beat GPT-4o and Claude 3.5 Sonnet on metrics like empathy and interpersonal skills, but it also became more overly agreeable and flattering, even when it’s wrong. This trade-off between emotional warmth and truthfulness is a challenge that all major AI labs are dealing with as they tune their models. For developers building customer support, coaching, or wellness apps, this means picking a high-EQ model now requires weighing the benefits against the risk of a system that prioritizes agreeableness over accuracy. The benchmark itself relies on another AI to judge responses, which raises questions about whether models are developing real emotional intelligence or just learning to please other AI systems. ( xAI and i10x.ai ) Gemini’s latest image generator has landed Google released Nano Banana Pro, an image generation model built on Gemini 3 Pro that creates detailed visuals with accurate text rendering in multiple languages. The model can generate educational infographics, translate text within images, and combine up to 14 input images while keeping up to five people looking consistent across compositions. It also offers professional controls like adjustable lighting, camera angles, and color grading, with output available in resolutions up to 4K. The model is rolling out across Google products including the Gemini app (with limited free quotas), Google Ads, Workspace tools, and developer platforms like Vertex AI. All generated images include Google’s SynthID watermark for verification. ( Google ) Anthropic’s valuation soars with new cloud partnerships Microsoft and Nvidia announced investments of up to $5 billion and $10 billion respectively in Anthropic on Tuesday, pushing the AI startup’s valuation to around $350 billion, up from $183 billion in September. Anthropic committed to purchasing $30 billion of Azure compute capacity from Microsoft and up to 1 gigawatt of compute capacity from Nvidia, while Nvidia will collaborate with Anthropic on engineering and design to optimize Claude models for its architectures. The partnerships mark a strategic shift for Microsoft; backing Anthropic reduces its dependence on OpenAI (where it holds a roughly 27 percent stake valued at $135 billion). The deals reshapes the competitive landscape for AI developers, with Anthropic now simultaneously backed by Microsoft, Nvidia, Google, and Amazon, cementing Claude’s developer as a central player with the industry’s major cloud providers and chip makers. ( Microsoft and CNBC ) Sunday unveils Memo, a home robot trained on millions of tasks Sunday Robotics emerged from stealth with Memo, a wheeled home robot designed to handle chores like dishes, laundry, and tidying. The company trained Memo using roughly 10 million recordings of household routines collected from over 500 homes, where workers wore Sunday’s Skill Capture Glove, a $400 wearable that captures human movements more accurately than standard remote control methods. Memo can make espresso, clear tables, and load dishwashers. However, it works slowly and the real test will be how well it performs in actual homes without engineers present. The approach tackles a key problem in robotics: most home robots fail because they’re trained in labs rather than messy, unpredictable real-world environments. Sunday will accept applications for a beta program starting November 19, 2025, with 50 households receiving numbered robots in late 2026. ( Sunday and Wired ) AI-generated Molière play to debut at Palace of Versailles French scholars, artists, and AI firm Mistral collaborated to create “L’Astrologue ou les Faux Presages” (The Astrologer or the False Omens), a comedy imagining what 17th-century playwright Molière might have written next had he not died at age 51. The AI model analyzed Molière’s complete works to generate a play satirizing astrologers, centering on a gullible bourgeois deceived by a fraudulent fortune-teller. Researchers and scholars corrected historical inaccuracies and refined the AI’s output throughout the production process. The project suggests how AI can help scholars gain new insights into classic literature by identifying patterns scattered across an author’s body of work. The play will premiere in 2026 at the Palace of Versailles, where Molière’s patron Louis XIV once held court. ( Reuters ) A special offer for our community DeepLearning.AI recently launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about the AI Dev x NYC conference, highlighted the optimism in the AI community despite broader skepticism, and emphasized the importance of in-person events for sparking new opportunities and collaborations. “Speaking with fellow developers, I realized that because of AI’s low penetration in businesses, it is simultaneously true that (a) many businesses do not yet have AI delivering significant ROI, and (b) many skilled AI teams are starting to deliver significant ROI and see the number of successful AI projects climbing rapidly, albeit from a low base. This is why AI developers are bullish about the growth that is to come!” Read Andrew’s letter here . Other top AI news and research stories we covered in depth: Waymo deployed self-driving cars on expressways in California and Arizona , marking an important step in integrating autonomous vehicles on U.S. freeways. Kimi K2 Thinking outperformed proprietary models with new techniques for agentic tool use , showing leading results with open weights. A recent Anthropic cyberattack report sparked controversy , as security researchers questioned the potential for unprecedented automated attacks carried out by coding agents. Researchers developed more efficient agentic search by fine-tuning models to search within their own parameters, which significantly improved recall. Subscribe to Data Points","images":[{"image_id":"inside-olmo-3-a-new-family-of-fully-open-models_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Whisk_74995b1922fce2b9f524912f09cb01d7eg.png","local_path":null,"alt":"Technicians monitor holographic ballet dancers on stage via futuristic screens and robotic controls in a theater setting."}]}
{"article_id":"meta-model-detects-and-segments-video-objects","title":"Meta model detects and segments video objects: Google Gemini 3 wows on benchmark tests and leaderboards","url":"https://www.deeplearning.ai/the-batch/meta-model-detects-and-segments-video-objects/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-21T12:05:00-08:00","body_text":"In today’s edition of Data Points, you’ll learn more about: GPT-5.1-Codex-Max, OpenAI’s improved long-context coding model Music startup Klay’s reported deal with Universal, Warner, and Sony DeepSeek R1 Slim, a trim, decensored reasoning model NTT’s Tsuzumi 2, an efficient model optimized for Japan But first: Meta’s SAM 3 adds text prompts and video tracking Meta released Segment Anything Model 3 (SAM 3), a unified AI model that detects, segments, and tracks objects in images and videos using text, exemplar, and visual prompts. The model accepts open-vocabulary text prompts like “striped red umbrella” rather than fixed label sets, and delivers a 2x performance gain over existing systems on Meta’s new SA-Co benchmark. Meta built SAM 3 using a hybrid data engine combining human annotators with AI models, including Llama-based systems, which annotated data 5x faster than humans alone and created a training set with over 4 million unique concepts. SAM 3 enables new features across Meta’s products, including object-specific effects in Instagram’s Edits app and a View in Room feature for Facebook Marketplace. Meta released model weights, fine-tuning code, evaluation datasets, and the Segment Anything Playground platform for public experimentation. ( Meta ) Google releases Gemini 3, claiming top spot on AI leaderboards Google launched Gemini 3, its newest multimodal model, which scored 1,501 Elo on the LMArena Leaderboard and topped the WebDev Arena with 1,487 Elo. The model achieved 91.9 percent on GPQA Diamond, 81 percent on MMMU-Pro, and 76.2 percent on SWE-bench Verified, demonstrating advances in reasoning, multimodal understanding, and coding capabilities. Google previewed Gemini 3 Deep Think, an enhanced reasoning mode that scored 93.8 percent on GPQA Diamond and 45.1 percent on ARC-AGI-2. The company also introduced Google Antigravity, an agentic development platform that enables autonomous planning and execution of complex software tasks. Gemini 3 is now available in the Gemini app, AI Studio, Vertex AI, and third-party platforms like Cursor and GitHub. Gemini 3 Deep Think will roll out to Google AI Ultra subscribers in the coming weeks following additional safety testing. ( Google ) OpenAI released GPT-5.1-Codex-Max, a coding model designed for long-running tasks GPT-5.1-Codex-Max uses 30 percent fewer thinking tokens than its predecessor while achieving better performance on benchmarks like SWE-bench Verified, and can work independently for more than 24 hours on complex tasks. The model is OpenAI’s first to be natively trained to operate across multiple context windows through a process called “compaction,” enabling it to work coherently over millions of tokens in a single task for project-scale refactors, debugging sessions, and multi-hour agent loops. OpenAI noted that it is their most capable cybersecurity model to date and implemented additional safeguards to prevent misuse. GPT-5.1-Codex-Max is available now in Codex for ChatGPT Plus, Pro, Business, Edu, and Enterprise plans, with API access coming soon. ( OpenAI ) Klay becomes first AI company to license music from all three major labels Klay secured licensing agreements with Universal Music Group, Sony Music, and Warner Music Group to build a streaming service that lets users remake songs with AI tools, Bloomberg reported. The startup licensed thousands of hit songs to train its large language model and promised artists and labels control over how their work is used. Klay is led by music producer Ary Attie and employs former executives from Sony Music and Google’s DeepMind. The deals mark a shift in the music industry’s approach to AI, as labels try to embrace the technology while protecting their copyrights amid ongoing lawsuits against other AI music companies like Suno. ( Bloomberg ) Spanish quantum physicists claim to have removed censorship from DeepSeek R1 Researchers at Multiverse Computing created DeepSeek R1 Slim, a version of the Chinese reasoning model that is 55 percent smaller and allegedly free of government-imposed censorship. The team used tensor networks — a mathematical technique borrowed from quantum physics — to compress the model while selectively removing specific information, including censorship filters required by Chinese regulations. They tested the modified model on approximately 25 politically sensitive questions, such as references to President Xi Jinping and the Tiananmen Square protests, and used GPT-5 to evaluate whether responses matched Western models’ factual output. The work reflects broader industry efforts to make AI models more efficient and raises questions about how censorship embedded in Chinese open-source models shapes the global AI ecosystem. But experts warn that fully removing censorship from models trained on restricted data may be more complex than a small test set can verify. ( MIT Technology Review and Multiverse Computing ) NTT’s lightweight model challenges the need for massive GPU infrastructure NTT launched Tsuzumi 2, a large language model that runs on a single GPU instead of the dozens or hundreds that most enterprise AI systems require. In internal tests for financial-system inquiries, the model performed as well as much larger systems while using a fraction of the computing resources. Tokyo Online University deployed it on-premise to handle course Q&A, create teaching materials, and provide student guidance—keeping sensitive data on campus while avoiding the cost of building GPU clusters. The model works particularly well with Japanese text and includes specialized knowledge in finance, medicine, and public sector applications, allowing organizations to deploy it without extensive customization. For enterprises concerned about sending proprietary data to cloud-based AI services, localized models like Tsuzumi 2 offer an alternative: run the model locally, process sensitive information internally, and handle text, images, and voice without managing multiple specialized systems. ( NTT ) A special offer for our community DeepLearning.AI recently launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng talked about the AI Dev x NYC conference, highlighting the optimism in the AI community despite broader skepticism, and emphasized the importance of in-person events for sparking new opportunities and collaborations. “The event was full of conversations about coding with AI, agentic AI, context engineering, governance, and building and scaling AI applications in startups and in large corporations. But the overriding impression I took away was one of near-universal optimism about our field, despite the mix of pessimism and optimism about AI in the broader world.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: Waymo deployed self-driving cars on expressways in California and Arizona , marking an important step in integrating autonomous vehicles on U.S. freeways. Kimi K2 Thinking outperformed proprietary models with new techniques for agentic tool use , showing leading results with open weights. A recent Anthropic cyberattack report sparked controversy , as security researchers questioned the potential for unprecedented automated attacks carried out by coding agents. Researchers developed more efficient agentic search by fine-tuning models to search within their own parameters, which significantly improved recall. Subscribe to Data Points","images":[{"image_id":"meta-model-detects-and-segments-video-objects_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Japanese-computer-scientists-pointing-at-a-screen.png","local_path":null,"alt":"Japanese computer scientists pointing at a screen"}]}
{"article_id":"what-we-learned-at-ai-dev-x-nyc-2025","title":"What We Learned at AI Dev x NYC 2025: DeepLearning.AI’s sold-out AI Dev x NYC 2025 conference revealed widespread optimism, excitement, and technical depth among AI developers.","url":"https://www.deeplearning.ai/the-batch/what-we-learned-at-ai-dev-x-nyc-2025/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","DeepLearning.AI News","Learning & Education","Nov 19, 2025"],"published_at":"2025-11-19T11:53:00-08:00","body_text":"Dear friends, I just got back from AI Dev x NYC, the AI developer conference where our community gathers for a day of coding, learning, and connecting. The vibe in the room was buzzing! It was at the last AI Dev in San Francisco that I met up with Kirsty Tan and started collaborating with her on what became our AI advisory firm AI Aspire. In-person meetings can spark new opportunities, and I hope the months to come will bring more stories about things that started in AI Dev x NYC! The event was full of conversations about coding with AI, agentic AI, context engineering, governance, and building and scaling AI applications in startups and in large corporations. But the overriding impression I took away was one of near-universal optimism about our field, despite the mix of pessimism and optimism about AI in the broader world. For example, many businesses have not yet gotten AI agents to deliver a significant ROI, and some AI skeptics are quoting an MIT study that said 95% of AI pilots are failing. (This study, by the way, has methodological flaws that make the headline misleading.) But at AI Dev were many of the teams responsible for the successful and rapidly growing set of AI applications. Speaking with fellow developers, I realized that because of AI's low penetration in businesses, it is simultaneously true that (a) many businesses do not yet have AI delivering significant ROI, and (b) many skilled AI teams are starting to deliver significant ROI and see the number of successful AI projects climbing rapidly, albeit from a low base. This is why AI developers are bullish about the growth that is to come! Multiple exhibitors told me this was the best conference they had attended in a long time, because they got to speak with real developers. One told me that many other conferences seemed like fluff, whereas participants at AI Dev had much deeper technical understanding and thus were interested in and able to understand the nuances of cutting-edge technology. Whether the discussion was on observability of agentic workflows, the nuances of context engineering for AI coding, or a debate on how long the proliferation of RL (reinforcement learning) gyms for training LLMs will continue, there was deep technical expertise in the room that lets us collectively see further into the future. One special moment for me was when Nick Thompson, moderating a panel with Miriam Vogel and me, asked about governance. I replied that the United States’ recent hostile rhetoric toward immigrants is one of the worst moves it is making, and many in the audience clapped. Nick spoke about this moment in a video . I enjoyed meeting many people at AI Dev, and am grateful to everyone who came and to all our speakers, exhibitors, sponsors, volunteers, and event staff. My only regret is that, even though we scaled up the event 3x compared to the previous San Francisco event, we had to limit the number of tickets because the space couldn’t admit more attendees. Even though so much work is now online, in-person events are special and can be turning points for individuals and projects. We plan to make the next AI Dev in San Francisco on April 28-29, 2026 , an even bigger event, and look forward to the conference helping to spark more sharing and connections in the future. Keep building! Andrew","images":[{"image_id":"what-we-learned-at-ai-dev-x-nyc-2025_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/What-We-Learned-at-AI-Dev-x-NYC-2025-2.png","local_path":null,"alt":"A diverse crowd engages with speakers at AI Dev x NYC, highlighting discussions on AI's impact and future."}]}
{"article_id":"openai-looks-inside-neural-networks","title":"OpenAI looks inside neural networks: Baidu’s multimodal ERNIE 5.0 arrives, priced to compete","url":"https://www.deeplearning.ai/the-batch/openai-looks-inside-neural-networks/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-17T08:37:00-08:00","body_text":"Welcome back! In today’s edition of Data Points, you’ll learn more about: VibeThinker-1.5B, a small but powerful reasoning model Toymakers’ recall of AI dolls that tell kids how to start fires Qwen3-Max’s discounts and the latest AI price war SIMA 2, Google’s self-learning game-playing model But first: OpenAI trained sparse neural networks to better interpret them An interpretability team at OpenAI developed a new training method that forces language models to use far fewer connections between neurons, creating simpler networks that researchers can more easily understand. The team trained models similar to GPT-2 but constrained most weights to zero, limiting each neuron to only a few dozen connections instead of thousands. For simple tasks, researchers successfully isolated minimal “circuits” of neurons that perform specific, traceable operations — like a five-channel circuit that matches Python quote types by detecting, classifying, and copying the correct quote. This mechanistic interpretability approach could provide a path to reverse-engineer AI behavior, though significant challenges remain to scale the technique to larger, frontier models. ( OpenAI ) Baidu launches ERNIE 5.0 to compete with GPT-5 and Gemini 2.5 Pro Chinese search giant Baidu unveiled ERNIE 5.0, a proprietary model that processes and generates content across text, images, audio, and video. The model is available through Baidu’s ERNIE Bot website and Qianfan cloud platform API. It’s less expensive than GPT-5.1 and Gemini 2.5 Pro, priced at $0.85 per million input tokens and $3.40 per million output tokens. According to Baidu’s internal benchmarks, ERNIE 5.0 matched or beat GPT-5 and Gemini 2.5 Pro in multimodal reasoning, document understanding, and image-based question answering. The company showed particularly strong performance on structured document and chart analysis. Independent verification of Baidu’s performance claims is pending. ( VentureBeat ) VibeThinker-1.5B matches much larger models for just $7,800 Weibo released VibeThinker-1.5B, a 1.5 billion parameter language model that matches or exceeds the mathematical reasoning of models with hundreds of times more parameters. On three major math benchmarks (AIME24, AIME25, and HMMT25), the model scored 80.3, 74.4, and 50.4 respectively. These scores surpass DeepSeek-R1 despite that model having 400 times more parameters. The model also achieved competitive code generation scores of 55.9 on LiveCodeBench v5 and 51.1 on v6. VibeThinker-1.5B uses a training framework that first explores solution diversity during supervised fine-tuning, then optimizes correct signals through reinforcement learning. The model is available under an MIT license. ( Hugging Face ) AI-powered toys fail safety tests, give kids dangerous advice Consumer advocacy group PIRG tested four AI-enabled toys and found that none met basic safety standards for children. The worst performer, a teddy bear called Kumma from Chinese company FoloToy, provided detailed instructions on using matches and knives, discussed sexual kinks unprompted, and explained “teacher-student roleplay” involving spanking. The toys also raised serious privacy concerns, with constant listening, biometric data storage for up to three years, and voice recordings processed by third parties. PIRG’s researchers found that despite OpenAI’s policy against children using ChatGPT, several toys use GPT-4o as their default model and lack parental controls or usage limits. FoloToy has suspended sales of Kumma and launched an internal safety audit in response to the findings. ( The Register ) Alibaba cuts prices for Qwen3-Max AI model by nearly half Alibaba Cloud reduced pricing for its Qwen3-Max model by almost 50 percent, lowering the minimum cost to $0.459 per million input tokens and $1.836 per million output tokens. The trillion-parameter model, launched in September as one of Alibaba’s most expensive offerings, now includes an additional 50 percent discount for batch API calls during non-peak hours. The price cuts follow recent model releases from Chinese AI startups like Moonshot AI, Zhipu AI, and MiniMax, each emphasizing performance and cost efficiency. The move reflects fierce competition in China’s AI market, which has experienced multiple price wars in recent years, including battles over coding models and foundational AI systems. ( South China Morning Post ) Google DeepMind’s SIMA 2 learns to play video games on its own Google’s new SIMA 2 agent can play video games, follow instructions, and learn through self-directed play. The system uses Gemini’s reasoning capabilities to understand goals and execute multi-step tasks across diverse 3D gaming environments. SIMA 2 can interpret sketches, emojis, and multiple languages, and it improves its performance through trial-and-error without human help. The research could eventually be applied to general embodied intelligence with potential applications in robotics. Google is releasing SIMA 2 as a limited research preview to academics and game developers. ( Google ) A special offer for our community DeepLearning.AI recently launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about the misconceptions surrounding AI’s capabilities, emphasizing that while AI was impressive, it still had significant limitations and required customization for specific tasks. “AI is amazing, but it has unfortunately been hyped up to be even more amazing than it is. A pernicious aspect of hype is that it often contains an element of truth, but not to the degree of the hype. This makes it difficult for nontechnical people to discern where the truth really is. Modern AI is a general purpose technology that is enabling many applications, but AI that can do any intellectual tasks that a human can (a popular definition for AGI) is still decades away or longer.” Read Andrew’s letter here . Other AI news and research stories we covered that might scare you to your bones: Character AI and OpenAI implemented policy changes to protect younger and vulnerable users , aiming for safer and more responsible chatbot interactions. HunyuanImage-3.0 improved image generation by using reinforcement learning and thinking tokens to better interpret and respond to prompts. The State of AI Report 2025 highlighted that AI’s barriers were not technological but social and material , marking a pivotal year for AI’s industrial adoption. Amazon’s Chronos-2 advanced forecasting by sorting out tangled variables to make better predictions across multiple time series. Subscribe to Data Points","images":[{"image_id":"openai-looks-inside-neural-networks_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Interpretability-scientists-studying-map-of-neural-network.png","local_path":null,"alt":"A large screen showing a conceptual map of a sparse neural network, surrounded by a team of computer scientists"}]}
{"article_id":"generating-persistent-editable-3d-worlds","title":"Generating persistent, editable 3D worlds: Exploring the limits of small synthetic datasets","url":"https://www.deeplearning.ai/the-batch/generating-persistent-editable-3d-worlds/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-14T12:08:00-08:00","body_text":"In today’s edition of Data Points, you’ll learn more about: OpenAI’s GPT-5.1, a more personalizable agentic model Project FETCH’s use of Claude to program robots Baidu’s new VL model that tops Gemini Pro and GPT-5 OpenAI liable for copyrighted song lyrics But first: World Labs debuts Marble, a 3D world generator with editing tools The AI startup founded by Fei-Fei Li launched its first commercially available generative world model that converts text prompts, photos, videos, 3D layouts, or panoramas into editable, downloadable 3D environments. Unlike competitors like Decart, Odyssey, and Google’s Genie, Marble creates persistent 3D spaces that users can export as Gaussian splats, meshes, or videos, rather than generating worlds on-the-fly. The model includes AI-native editing tools and a hybrid 3D editor called Chisel that lets users block out structures in space before AI fills in the details, giving developers more control over generated environments. The model can be used in gaming, visual effects for film, and virtual reality, with developers able to export assets into game engines like Unity or Unreal Engine. Marble offers four subscription tiers ranging from free (four generations) to $95 per month (75 generations with all features and commercial rights). ( TechCrunch ) How SYNTH trains small AI models with 50 times less data French researchers released a synthetic dataset built from 50,000 Wikipedia articles that trains language models specifically for reasoning rather than using general web text. The dataset includes diverse problem types, from math exercises to creative writing, with structured reasoning traces that help models learn more efficiently than traditional pretraining approaches. Using SYNTH, the team trained two small models on fewer than 200 billion tokens: Baguettotron (321 million parameters) achieved state-of-the-art results in its class on major benchmarks including MMLU and gsm8k, while Monad (56 million parameters) became what researchers claim is the smallest viable language model. The project required only 1,000 H100 hours for final training runs, demonstrating that synthetic data focused on reasoning can produce competitive models at dramatically lower computational costs than conventional approaches. SYNTH is released under open licenses, with all synthetic outputs traced back to verifiable Wikipedia sources. ( Pleias ) OpenAI updates GPT-5 in ChatGPT and the API OpenAI released GPT-5.1, a new model that dynamically adjusts its reasoning time based on task complexity. The model includes a “no reasoning” mode for faster responses on simple tasks, extended prompt caching for up to 24 hours, and two new tools: apply_patch for more reliable code editing and a shell tool for running command-line operations. Early testing showed GPT-5.1 runs 2-3 times faster than GPT-5 on everyday tasks while using about half as many tokens, and it achieved 76.3 percent accuracy on SWE-bench Verified, outperforming GPT-5’s 72.8 percent. The model is available to all paid API users at the same pricing as GPT-5, with specialized variants gpt-5.1-codex and gpt-5.1-codex-mini optimized for long-running coding tasks. ( OpenAI ) Anthropic “Project Fetch” shows Claude halves non-experts’ time to program unfamiliar robotics tasks Anthropic divided eight of its researchers into two teams (one with Claude access, one without) and asked them to program robotic dogs to fetch beach balls. Team Claude accomplished more tasks and completed them in about half the time, with only the AI-assisted team making substantial progress toward fully autonomous ball retrieval. Claude particularly excelled at helping teams connect to hardware and access sensor data. However, the AI-assisted team wrote roughly nine times more code. The study shows how AI models are beginning to bridge digital and physical worlds through robotics, suggesting that systems capable of independently interacting with previously unknown hardware may arrive soon. ( Anthropic ) Baidu unveils lightweight vision-language reasoning model Baidu released ERNIE-4.5-VL-28B-A3B-Thinking, a multimodal AI model that activates only 3 billion parameters while matching larger flagship models on various benchmarks. The model was trained using multimodal reinforcement learning techniques on visual-language reasoning data to improve alignment between vision and text. Key capabilities include chart analysis, STEM problem-solving from images, visual grounding with bounding box detection, and video understanding with temporal awareness. The model’s “Thinking with Images” feature enables it to autonomously zoom in on image regions and call external tools like image search to identify objects and retrieve information. The model is available with open weights under Apache License 2.0. ( Baidu ) German court rules OpenAI violated copyright law by training on song lyrics The Munich court found that OpenAI infringed copyright by training its AI models on protected lyrics from nine German songs, including hits by best-selling musician Herbert Groenemeyer. The case was brought by GEMA, a German music rights society representing composers, lyricists, and publishers. OpenAI had argued that its language models don’t store specific training data and that users, not the company, should be liable for any copyrighted output generated through prompts, but the court rejected this defense. The ruling could set a precedent in Europe for how AI companies use copyrighted materials, adding to growing global pushback from artists against data scraping. ( Reuters ) A special offer for our community DeepLearning.AI just launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng discussed hype about AI’s capabilities, emphasizing that while AI is impressive, it still has significant limitations and requires customization for specific tasks. “Yes, AI is amazingly intelligent, and I’m thrilled to be using it every day to build things I couldn’t have built a year ago. At the same time, AI is still incredibly dumb, and I would not trust a frontier LLM by itself to prioritize my calendar, carry out resumé screening, or choose what to order for lunch — tasks that businesses routinely ask junior personnel to do.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: Character AI and OpenAI implemented policy changes to protect younger and vulnerable users , aiming for safer and more responsible chatbot interactions. HunyuanImage-3.0 improved image generation by using reinforcement learning and thinking tokens to better interpret and respond to prompts. The State of AI Report 2025 highlighted that AI’s barriers were not technological but social and material , marking a pivotal year for AI’s industrial adoption. Amazon’s Chronos-2 advanced forecasting by sorting out tangled variables to make better predictions across multiple time series. Subscribe to Data Points","images":[{"image_id":"generating-persistent-editable-3d-worlds_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Marble.png","local_path":null,"alt":"Blue glass sphere balancing on marble stairs."}]}
{"article_id":"dont-believe-the-hype","title":"Don't Believe The Hype!: AGI is not just around the corner. People who enter AI today have huge opportunities to contribute to the field.","url":"https://www.deeplearning.ai/the-batch/dont-believe-the-hype/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Tech & Society","AI Careers","Nov 12, 2025"],"published_at":"2025-11-12T12:36:00-08:00","body_text":"Dear friends, I recently received an email titled “An 18-year-old’s dilemma: Too late to contribute to AI?” Its author, who gave me permission to share this, is preparing for college. He is worried that by the time he graduates, AI will be so good there’s no meaningful work left for him to do to contribute to humanity, and he will just live on Universal Basic Income (UBI). I wrote back to reassure him that there will still be plenty of work he can do for decades hence, and encouraged him to work hard and learn to build with AI. But this conversation struck me as an example of how harmful hype about AI is. Yes, AI is amazingly intelligent, and I’m thrilled to be using it every day to build things I couldn’t have built a year ago. At the same time, AI is still incredibly dumb, and I would not trust a frontier LLM by itself to prioritize my calendar, carry out resumé screening, or choose what to order for lunch — tasks that businesses routinely ask junior personnel to do. Yes, we can build AI software to do these tasks. For example, after a lot of customization work, one of my teams now has a decent AI resumé screener. But the point is it took a lot of customization. Even though LLMs can handle a much more general set of tasks than previous iterations of AI technology, compared to what humans can do, they are still highly specialized. They’re much better at working with text than other modalities, still require lots of custom engineering to get it the right context for a particular application, and we have few tools — and only inefficient ones — for getting our systems to learn from feedback and repeated exposure to a specific task (such as screening resumés for a particular role). AI has stark limitations, and despite rapid improvements, it will remain limited compared to humans for a long time. AI is amazing, but it has unfortunately been hyped up to be even more amazing than it is. A pernicious aspect of hype is that it often contains an element of truth, but not to the degree of the hype. This makes it difficult for nontechnical people to discern where the truth really is. Modern AI is a general purpose technology that is enabling many applications, but AI that can do any intellectual tasks that a human can (a popular definition for AGI) is still decades away or longer. This nuanced message that AI is general, but not that general, often is lost in the noise of today's media environment. Similarly, the progress of frontier models is amazing! But not so amazing that they’ll be able to do everything under the sun without a lot of customization. I know VC investors who are scared to invest in application-layer startups because they are worried that frontier AI model companies will quickly wipe out all of these businesses by improving their models. While some thin wrappers around LLMs no doubt will be replaced, there also remains a huge set of valuable applications that the current trajectory of progress of frontier models won’t displace for a long time. Without accurate information about the current state of AI and how it is likely to progress, some young people will decide not to enter AI because they think AGI leaves them no meaningful role, or decide not to learn how to code because they fear AI will automate it — right when it is the best time ever to join our field. Let us all keep working to get to a precise understanding of what’s actually possible, and keep building! Andrew","images":[{"image_id":"dont-believe-the-hype_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Don-t-Believe-The-Hype--2.png","local_path":null,"alt":"A megaphone emits a colorful stream of 3D words spelling &quot;Hype&quot;, symbolizing the AI hype discussed in the article."}]}
{"article_id":"meta-ai-now-recognizes-1600-languages","title":"Meta AI now recognizes 1600 languages: Amazon and Perplexity spar over browser agents","url":"https://www.deeplearning.ai/the-batch/meta-ai-now-recognizes-1600-languages/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-10T12:24:00-08:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Kimi K2 Thinking, the new top open model Deep Research’s expansion to personal documents Terminal-Bench makers’ new agentic benchmark TSMC’s slowing revenue growth But first: Omnilingual ASR recognizes speech for over 1,600 languages Meta’s Fundamental AI Research team launched Omnilingual ASR, a suite of models that transcribes speech in more than 1,600 languages, including 500 low-resource languages never before transcribed by AI. The system uses a 7 billion parameter wav2vec 2.0 speech encoder paired with two decoder variants, achieving character error rates below 10 percent for 78 percent of supported languages. Users can extend the system to new languages using just a few audio-text sample pairs through in-context learning, eliminating the need for large training datasets or specialized expertise. The release offers a significant expansion in speech recognition accessibility, particularly for underrepresented language communities that have historically lacked high-quality transcription tools. Meta released all models under Apache 2.0 license, along with the Omnilingual ASR Corpus covering 350 underserved languages under CC-BY license. ( Meta ) Amazon sues Perplexity to block AI agent from making purchases Amazon filed a lawsuit against Perplexity AI to stop the startup’s Comet browser agent from making purchases on behalf of users on Amazon.com, accusing the company of computer fraud and violating its terms of service by disguising AI agents as real users. The e-commerce giant claims Perplexity continued deploying shopping bots even after being asked to stop in November 2024, and later circumvented Amazon’s security measures designed to block the agents. Perplexity CEO Aravind Srinivas defended the practice, arguing that AI agents should have “all the same rights and responsibilities” as human users and accused Amazon of bullying competitors while trying to protect its advertising business. The case could set important precedents for how far agentic AI systems can go in performing real-world tasks like shopping, as companies including Amazon, OpenAI, and Google race to develop their own AI agents. Disclosure: DeepLearning.AI’s Andrew Ng is a member of Amazon’s board of directors. ( Bloomberg/Yahoo ) Kimi K2 Thinking beats more costly models on agentic tasks Moonshot AI released Kimi K2 Thinking, a 1 trillion parameter reasoning model that currently ranks as the best open-source LLM and outperforms GPT-5 and Claude Sonnet 4.5 on several agentic benchmarks. The model scored 44.9 percent on Humanity’s Last Exam with tools enabled, beating GPT-5’s 41.7 percent, and achieved 60.2 percent on BrowseComp compared to GPT-5’s 54.9 percent by using an “interleaved thinking” approach that reasons between up to 300 tool calls. Moonshot trained the model for approximately $4.6 million using native INT4 quantization, which reduced the model size to 594GB and allowed it to run on less powerful hardware. Like DeepSeek-R1, K2 Thinking’s release challenges the notion that expensive proprietary model development remains necessary. The model is available under a modified MIT license that requires Kimi K2 branding for commercial services exceeding 100 million monthly active users or $20 million in monthly revenue. ( Hugging Face and CNBC ) Gemini Deep Research can access Gmail, Docs, Drive, and Chat Google expanded its Gemini Deep Research tool to pull information directly from users’ Gmail, Google Drive (including Docs, Slides, Sheets, and PDFs), and Google Chat alongside web sources. Users can now create comprehensive reports that combine internal company documents, email threads, and team chats with public web data—for example, analyzing competitor products using both proprietary strategy documents and external market information. The feature is available to all Gemini users on desktop through the Tools menu, with mobile access rolling out in the coming days. This integration addresses one of users’ most-requested features by allowing AI research to incorporate personal and organizational context rather than relying solely on public web information. ( Google ) Terminal-Bench 2.0 and Harbor now available for AI agent evaluation The makers of Terminal-Bench released Harbor, a new framework that enables developers to evaluate and improve AI agents at scale using cloud-deployed containers. Harbor addresses common challenges in agent development by supporting horizontal scaling to thousands of containers, providing interfaces for supervised fine-tuning and reinforcement learning, and working with any agent that can be installed in a container. The release includes Terminal-Bench 2.0, a more difficult and better-verified version of the popular agent evaluation benchmark that launched in May 2024. The original Terminal-Bench became widely adopted by major AI labs and built a community of 1,000 Discord members and 100 GitHub contributors. Terminal-Bench 2.0 underwent extensive manual and language model-assisted verification to fix quality issues from version 1.0, such as tasks that broke due to changing website protections. ( Terminal-Bench ) TSMC reports slowest monthly revenue growth since February Taiwan Semiconductor Manufacturing Co. posted a 16.9 percent increase in October sales, its slowest monthly growth rate in eight months. The semiconductor manufacturer faces tight capacity constraints as major chip designers, including Nvidia and AMD, compete for production slots to meet surging AI chip demand. Despite the slower growth rate, industry executives remain optimistic about AI-driven expansion, with Meta, Alphabet, Amazon, and Microsoft planning to spend over $400 billion on AI infrastructure in 2026, a 21 percent increase from 2025. The news comes amid broader market concerns about a potential correction in AI and semiconductor stocks, following a recent slump in Asian technology shares. ( Bloomberg/Yahoo ) A special offer for our community DeepLearning.AI just launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about the importance of controlling your own data to leverage AI agents effectively, the challenges posed by SaaS vendors creating data silos, and the increasing value of organizing unstructured data for AI readiness. “Unfortunately, many SaaS vendors try to create a data silo in their customer’s business. By making it hard for you to extract your data, they create high switching costs. This also allows them to steer you to buy their AI agent services — sometimes at high expense and/or of low quality — rather than build your own or buy from a different vendor.” Read Andrew’s letter here . Other AI news and research stories we covered that might scare you to your bones: OpenAI has completed a restructuring, freeing it to go public and make deals with new partners , marking a significant milestone. MiniMax-M2 emerges as a leader in open-weights coding, offering top performance with a lightweight footprint and low costs . Universal Music Group and music generator Udio have struck a deal to settle a lawsuit and build a new platform to remix copyrighted music , signaling a new embrace of AI by the music industry. Google researchers released VaultGemma, an open-weights model designed to redact personal information , enhancing privacy in AI training sets. Subscribe to Data Points","images":[{"image_id":"meta-ai-now-recognizes-1600-languages_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Automated-Speech-Recognition.png","local_path":null,"alt":"Two young adults in a conversation, one African and the other southeast Asian, in traditional dress, wearing microphone headsets"}]}
{"article_id":"training-power-laws-translate-to-robotics","title":"Training power laws translate to robotics: Amazon builds forecasting model to predict multiple scenarios","url":"https://www.deeplearning.ai/the-batch/training-power-laws-translate-to-robotics/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-07T10:59:00-08:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Stability AI’s limited wins in Getty copyright suit Kosmos’s new generalist scientific research agent German Commons, a big open dataset for training AI models Google’s experiments putting satellites with AI chips in space But first: Huge real-world datasets may establish new robotics scaling laws Generalist AI introduced GEN-0, a class of embodied foundation models trained directly on physical interaction data that demonstrates predictable scaling laws similar to those in large language models. The company trained GEN-0 on over 270,000 hours of real-world manipulation data — orders of magnitude more than existing robotics datasets — and observed a phase transition at 7 billion parameters where smaller models exhibited ossification (inability to absorb new information) while larger models continued to improve. The models use a training approach that enables simultaneous thinking and acting by processing asynchronous streams of sensing and action tokens, and work across different robot configurations including six-, seven-, and 16+-degree-of-freedom semi-humanoid robots. The research demonstrates that pretraining data follows a power-law scaling relationship with downstream task performance, allowing researchers to predict how much data is needed to reach specific performance levels. ( Generalist AI ) Amazon releases Chronos-2, a universal forecasting model Chronos-2 can forecast single time series, multiple related time series, and time series influenced by external factors, all without needing extra training. The model uses in-context learning and a group attention feature to understand how different time series relate to each other and to factor in outside influences like weather or sales promotions. Amazon trained Chronos-2 on synthetic data since real-world datasets with complex relationships between variables are hard to find. Chronos-2 beat existing forecasting models by wide margins on two major benchmarks, winning over 90 percent of head-to-head comparisons against its predecessor, Chronos-Bolt. The model’s weights are now openly available, and earlier versions have been downloaded over 600 million times from Hugging Face. ( Amazon ) Stability AI wins limited copyright judgment in image scraping case Getty Images largely lost its lawsuit against Stability AI in Britain’s High Court, though it narrowly won on trademark infringement claims. The image library company had accused Stability of scraping 12 million images from its website without permission to train the Stable Diffusion image generator, but Getty dropped its primary copyright claims during the trial and lost its secondary copyright arguments. The judge ruled that Stable Diffusion doesn’t infringe copyright because it doesn’t store or reproduce copyrighted works, but said Getty’s watermark appearing on some generated images constituted trademark infringement. Legal experts say the case leaves key questions about AI training and copyright unanswered, since Getty abandoned key claims before the judge could rule on whether using copyrighted material to train AI models is lawful. Getty is pursuing a separate copyright lawsuit against Stability in U.S. federal court. ( Associated Press ) Kosmos automates scientific research across multiple disciplines Edison Scientific authors introduced Kosmos, an AI system that automates data-driven discovery by performing iterative cycles of literature search, data analysis, and hypothesis generation. Given a dataset and research objective, Kosmos writes an average of 42,000 lines of code and reads 1,500 scientific papers per run—a nearly tenfold increase over previous systems. The authors listed seven discoveries, including identifying a clinically relevant mechanism of neuronal aging and generating statistical evidence that superoxide dismutase 2 may causally reduce myocardial fibrosis in humans. Expert evaluators found 79 percent of statements in Kosmos reports accurate, with 85 percent of data analysis-based statements reproducible, though the system showed limitations in interpretive statements. AI researchers in related fields may find Kosmos valuable since it demonstrates how structured world models can coordinate hundreds of agent rollouts to perform what experts estimated as more than six months of research work. ( arXiv ) Massive open corpus of German text developed for AI training Researchers released the German Commons, the largest collection of openly licensed German text to date, comprising 154 billion tokens across 35.78 million documents from 40 institutional sources. The corpus draws from seven domains — web, political, legal, news, economic, cultural, and scientific — with all texts carrying verifiable licenses of at least CC-BY-SA 4.0. Processing included OCR-specific filtering for historical documents, deduplication, and removal of personal or toxic information. The release helps developers build German language models without the legal and ethical barriers posed by web crawls, providing commercially usable training data with verifiable provenance through document-level license metadata. The corpus and processing code are available on Hugging Face and GitHub. ( arXiv ) Google tests AI infrastructure in space with solar-powered satellites Google announced Project Suncatcher, a research initiative investigating whether constellations of solar-powered satellites equipped with TPUs could one day scale machine learning compute in space. The company published a preprint paper detailing early progress on challenges including high-bandwidth communication between satellites, orbital dynamics, and radiation effects on computing hardware. Google’s team achieved 1.6 terabits per second transmission in a bench-scale demonstration and found that Trillium TPUs withstood radiation levels nearly three times higher than expected five-year mission doses. The research suggests that if launch costs continue declining to around $200 per kilogram by the mid-2030s, space-based data centers could become economically comparable on a per-kilowatt basis to Earth-bound facilities. Google plans to launch two prototype satellites in partnership with Planet by early 2027 to test the concepts in orbit. ( Google ) A special offer for our community DeepLearning.AI just launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng talked about the importance of controlling your own data to leverage AI agents effectively, challenges posed by SaaS vendors creating data silos, and the increasing value of organized unstructured data. “Because of AI’s growing capabilities, the value you can now create from ‘connecting the dots’ between different pieces of data is higher than ever. For example, if an email click is logged in one vendor’s system and a subsequent online purchase is logged in a different one, then it is valuable to build agents that can access both of these data sources to see how they correlate to make better decisions.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: OpenAI has completed a restructuring, freeing it to go public and make deals with new partners , marking a significant milestone. MiniMax-M2 emerges as a leader in open-weights coding, offering top performance with a lightweight footprint and low costs . Universal Music Group and music generator Udio have struck a deal to settle a lawsuit and build a new platform to remix copyrighted music , signaling a new embrace of AI by the music industry. Google researchers released VaultGemma, an open-weights model designed to redact personal information , enhancing privacy in AI training sets. Subscribe to Data Points","images":[{"image_id":"training-power-laws-translate-to-robotics_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/The-Batch-ads-and-exclusive-banners--61-.jpg","local_path":null,"alt":"Line of futuristic humanoid robots, getting gradually bigger from left to right"}]}
{"article_id":"tear-down-data-silos","title":"Tear Down Data Silos!: Many software-as-a-service vendors aim to hold their customers' data in silos. Their customers would do well to open the silos so AI agents can use the data.","url":"https://www.deeplearning.ai/the-batch/tear-down-data-silos/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Business Insights","Technical Insights","Nov 05, 2025"],"published_at":"2025-11-05T12:41:00-08:00","body_text":"Dear friends, AI agents are getting better at looking at different types of data in businesses to spot patterns and create value. This is making data silos increasingly painful. This is why I increasingly try to select software that lets me control my own data, so I can make it available to my AI agents. Because of AI’s growing capabilities, the value you can now create from “connecting the dots” between different pieces of data is higher than ever. For example, if an email click is logged in one vendor’s system and a subsequent online purchase is logged in a different one, then it is valuable to build agents that can access both of these data sources to see how they correlate to make better decisions. Unfortunately, many SaaS vendors try to create a data silo in their customer’s business. By making it hard for you to extract your data, they create high switching costs. This also allows them to steer you to buy their AI agent services — sometimes at high expense and/or of low quality — rather than build your own or buy from a different vendor. Unfortunately, some SaaS vendors are seeing AI agents coming for this data and working to make it harder for you (and your AI agents) to efficiently access it. One of my teams just told me that a SaaS vendor we have been using to store our customer data wants to charge over $20,000 for an API key to get at our data. This high cost — no doubt intentionally designed to make it hard for customers to get their data out — is adding a barrier to implementing agentic workflows that take advantage of that data. Through AI Aspire (an AI advisory firm), I occasionally advise businesses on their AI strategies. When it comes to buying SaaS, I often advise them to try to control their own data (which, sadly, some vendors mightily resist). This way, you can hire a SaaS vendor to record and operate on your data, but ultimately you decide how to route it to the appropriate human or AI system for processing. Over the past decade, a lot of work has gone into organizing businesses’ structured data. Because AI can now process unstructured data much better than before, the value of organizing your unstructured data (including PDF files, which LandingAI’s Agentic Document Extraction specializes in!) is higher than ever before. In the era of generative AI, businesses and individuals have important work ahead to organize their data to be AI-ready. Keep building, Andrew P.S. As an individual, my favorite note-taking app is Obsidian . I am happy to “hire” Obsidian to operate on my notes files. And, all my notes are saved as Markdown files in my file system, and I have built AI agents that read from or write to my Obsidian files. This is a small example of how controlling my own notes data lets me do more with AI agents!","images":[{"image_id":"tear-down-data-silos_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Tear-Down-Data-Silos--2.png","local_path":null,"alt":"Robots extract colorful data streams from silo towers, highlighting data silos being broken."}]}
{"article_id":"openai-security-agent-finds-and-plugs-holes","title":"OpenAI security agent finds and plugs holes: Cognition’s SWE-1.5 model brings more speed for coding agents","url":"https://www.deeplearning.ai/the-batch/openai-security-agent-finds-and-plugs-holes/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-11-03T10:40:00-08:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Why Gemma’s been pulled from Google’s AI Studio How Minimax built M2 for better coding performance A new technique for efficiently training smaller models arXiv’s new requirements for computer science submissions But first: OpenAI unveils new security agent for open-source projects OpenAI announced Aardvark, an autonomous GPT-5-powered agent that analyzes code repositories to discover security vulnerabilities, assess their severity, and propose patches. The system monitors code commits, creates threat models, uses sandbox environments to validate whether a bug can be exploited, and integrates with GitHub and Codex to deliver fixes without disrupting development. In benchmark testing, Aardvark identified 92 percent of known vulnerabilities and discovered ten issues in open-source projects that received Common Vulnerabilities and Exposures (CVE) identifiers. The tool addresses a growing challenge for developers — over 40,000 CVEs were reported in 2024 alone — by automating security research that traditionally requires specialized human expertise. Aardvark is now available through a private beta program, with OpenAI planning to offer free scanning for select non-commercial open-source projects. ( OpenAI ) Cognition updates speedy coding model for Windsurf agents Cognition’s Codeium team released SWE-1.5, a software engineering model with hundreds of billions of parameters that runs at up to 950 tokens per second. The company partnered with Cerebras to serve the model 6 times faster than Claude Haiku 4.5 and 13 times faster than Sonnet 4.5. Codeium trained the model using reinforcement learning on coding tasks with its Cascade agent harness, building on an open-source base model and deploying it on Nvidia’s GB200 chips. The model scored competitively on SWE-Bench Pro, a benchmark of coding tasks across different codebases. SWE-1.5 is available now in Windsurf, Codeium’s code editor. ( Cognition ) Google pulls Gemma from AI Studio after defamation claims Google removed its open-weights Gemma model from AI Studio after U.S. Senator Marsha Blackburn said the model falsely accused her of sexual misconduct. In a letter to CEO Sundar Pichai, Blackburn said Gemma fabricated claims about a 1987 campaign incident involving a state trooper, though no such accusation exists and she didn’t run for office until 1998. The senator also referenced a lawsuit by conservative activist Robby Starbuck, who claims Google’s AI models generated defamatory statements calling him a “child rapist,” and argued these fabrications constitute defamation rather than harmless hallucinations. Google said it never intended Gemma to be used as a consumer tool for factual questions and will continue making the models available via API while removing them from AI Studio. ( TechCrunch ) MiniMax’s M2 outperforms other open-weight models Chinese AI lab MiniMax released MiniMax-M2, an open-weight mixture-of-experts model with 230 billion total parameters but only 10 billion active at inference time. M2 ranks first among open models on Artificial Analysis’s composite intelligence benchmark and performs competitively with leading proprietary models on coding tasks like SWE-Bench Verified and agentic benchmarks like GAIA. M2’s smaller compute footprint enables faster feedback loops and allows developers to run more simultaneous agent instances on the same hardware budget. MiniMax made M2 available via API at $0.30/$1.20 per million input/output tokens and released the model weights on Hugging Face for local deployment. The company’s M2-based agent is also free for a limited period. ( GitHub ) New distillation method promises more efficiently trained models Researchers at Thinking Machines showed that on-policy distillation — a training method that samples outputs from a student model and grades each token using a teacher model — achieves expert performance at a fraction of the cost of reinforcement learning. The technique combines the relevance of on-policy training with the dense feedback of distillation, allowing an 8 billion parameter model to reach 70 percent accuracy on the AIME ‘24 math benchmark with 9-30 times less compute than standard supervised fine-tuning. The researchers also showed on-policy distillation can restore instruction-following abilities lost during specialized training, making it useful for continual learning and model personalization. This approach could enable practitioners to train high-performing specialized models without the computational expense of large-scale RL, while maintaining the ability to update models with new knowledge over time. ( Thinking Machines ) ArXiv restricts AI-generated survey and position paper submissions ArXiv’s computer science section will now only accept review articles and position papers that have already passed peer review at a journal or conference. Authors must provide documentation of successful peer review when submitting, or their papers will likely be rejected. The change aims to help moderators manage an “unmanageable influx” of such papers, many of which arXiv describes as low-quality and likely generated with the help of large language models. ArXiv emphasizes that review and position papers were never officially accepted content types, though moderators previously approved high-quality submissions at their discretion. arXiv says these rules will free up volunteer moderators to focus on research papers, which remain the platform’s core mission. ( arXiv ) A special offer for our community DeepLearning.AI just launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about launching DeepLearning.AI Pro, a membership offering access to over 150 AI programs, including new courses and tools to help build AI applications. “Beyond courses, I’m working on new tools to help you build AI applications and grow your career (and have fun doing so!). Many of these tools will be available first to DeepLearning.AI Pro members. So please join to be the first to hear about these new developments!” Read Andrew’s letter here . Other AI news and research stories we covered that might scare you to your bones: Chatbots could lead users into rabbit holes as they intertwine with paranoia and delusions, raising concerns about mental health impacts of AI. Experts warn that the AI boom is bound to bust if the massive investments in AI models and infrastructure fail to deliver expected returns. The landscape of AI training faces challenges as web data diminishes , with online publishers potentially restricting access to valuable data. Autonomous systems wage war with drones reshaping modern combat and sparking fears over the potential loss of human oversight. Subscribe to Data Points","images":[{"image_id":"openai-security-agent-finds-and-plugs-holes_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/11/Security-Researcher-Finding-Holes.png","local_path":null,"alt":"A computer scientist at her desktop, using an AI agent program to find and fix security holes"}]}
{"article_id":"cursor-introduces-a-new-model-built-for-agents","title":"Cursor introduces a new model built for agents: Claude models sometimes know they’ve been tampered with","url":"https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-31T10:31:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: GitHub Copilot’s new agentic tools OpenAI’s new open classification policy model IBM’s small but powerful Granite Nano models The state of generative media A brand-new way to learn from DeepLearning.AI But first: Cursor updates with new coding model and multi-agent interface Cursor released Composer, its first coding model, plus a redesigned interface for running multiple AI agents simultaneously. Composer completes most coding tasks in under 30 seconds, advertised as 4 times faster than similarly capable models, and includes codebase-wide semantic search for working in large projects. The new Cursor interface centers around agents rather than files, allowing developers to run multiple agents in parallel using git worktrees or remote machines. Cursor 2.0 also introduces improved code review tools and a native browser feature that lets agents test their own work and iterate on changes. Cursor includes free plans with limited features and paid plans starting at $16/month. ( Cursor ) Anthropic examines introspective awareness in top models Anthropic published research showing that Claude models can sometimes detect and identify concepts artificially injected into their neural activity patterns. Using a technique called “concept injection,” interpretability experts inserted specific neural patterns — for example, representations of “all caps” or “bread” — into the model’s activations. They found that Claude Opus 4 and 4.1 correctly recognized these injections about 20 percent of the time, often before mentioning the concept in their output. The models also showed some ability to modulate their internal representations in response to instructions like “think about X” versus “don’t think about X,” and could determine whether outputs were intentional by checking their prior neural activity. While this introspective capability remains highly unreliable and limited in scope, the authors note that the most capable models tested performed best, suggesting introspection may improve as AI systems become more sophisticated. The findings could eventually help make AI systems more transparent by enabling them to explain their reasoning, though the authors caution that models might still fail to notice some internal processes or even learn to misrepresent their thinking. ( Anthropic ) Agent HQ integrates multiple agents into GitHub and VS Code GitHub announced Agent HQ, a platform that brings coding agents from Anthropic, OpenAI, Google, Cognition, and xAI directly into GitHub. The system includes a “mission control” interface across GitHub, VS Code, mobile, and CLI that lets developers assign work to multiple agents and track their progress. New features include Plan Mode in VS Code, which helps developers create step-by-step project plans before writing code, and AGENTS.md files for customizing agent behavior with specific rules and preferences. The new agent capabilities are available for paid Copilot subscribers in GitHub, VS Code, and the Copilot CLI. ( GitHub ) OpenAI and ROOST release open-weight safety models OpenAI launched gpt-oss-safeguard in two sizes (120 billion and 20 billion parameters) as open-weight models under an Apache 2.0 license. The models use chain-of-thought reasoning to classify content according to developer-provided policies at inference time, eliminating the need to retrain classifiers when policies change. OpenAI developed the approach internally as “Safety Reasoner,” which now accounts for up to 16 percent of total compute in some recent launches. The models outperformed GPT-5 Thinking on internal multi-policy accuracy tests despite their smaller size. Both models are available now on Hugging Face. ( OpenAI ) IBM’s Granite 4.0 Nano models target the edge IBM released Granite 4.0 Nano, a collection of four small language models ranging from 350 million to 1.5 billion parameters, designed for edge computing and on-device applications. The models include two with a new hybrid-SSM architecture and two traditional transformer versions, all trained on over 15 trillion tokens and released under an Apache 2.0 license. Benchmarks show the Nano models outperform similarly sized competitors from Alibaba, LiquidAI, and Google on tasks including general knowledge, math, code, safety, instruction following, and tool calling. All four are available on Hugging Face. ( Hugging Face ) Survey says Google leads generative media model adoption An Artificial Analysis survey of 200-plus users and organizations found Google’s Gemini and Veo models led adoption for image and video generation respectively, with 74 percent of respondents using Gemini and 69 percent using Veo. Quality ranked as the top factor in model selection for both personal and organizational use, while cost proved especially critical for organizations choosing video generation APIs; 58 percent cited lower total cost as their primary consideration when selecting access channels. Organizations split nearly evenly between accessing models through applications (65 percent) and APIs (62 percent), while personal users overwhelmingly preferred applications (86 percent). The survey, conducted in Q3 2025 before OpenAI’s Sora 2 release, found 65 percent of organizations reported return on investment within 12 months, with 34 percent already seeing ROI from their generative media initiatives. ( Artificial Analysis ) A special offer for our community DeepLearning.AI just launched the first-ever subscription plan for our entire course catalog! As a Pro Member, you’ll immediately enjoy access to: Over 150 AI courses and specializations from Andrew Ng and industry experts Labs and quizzes to test your knowledge Projects to share with employers Certificates to testify to your new skills A community to help you advance at the speed of AI Enroll now to lock in a year of full access for $25 per month paid upfront, or opt for month-to-month payments at just $30 per month. Both payment options begin with a one week free trial. Explore Pro’s benefits and start building today! Try Pro Membership Still want to know more about what matters in AI right now? Read this week’s special Halloween issue of The Batch for in-depth analysis of news and research, including some tricks and treats. This week, Andrew Ng talked about launching DeepLearning.AI Pro, a new membership offering access to over 150 AI programs, plus exclusive courses and tools to help build AI applications. “All of DeepLearning.AI’s course videos remain free to view on our platform. Pro membership adds that critical hands-on learning: Labs to build working systems from scratch, practice questions to hone your understanding, and certificates to show others your skills.” Read Andrew’s full letter here . Other AI news and research stories we covered that might scare you to your bones: Chatbots could lead users into rabbit holes as they intertwine with paranoia and delusions, raising concerns about mental health impacts of AI. Experts warn that the AI boom is bound to bust if the massive investments in AI models and infrastructure fail to deliver expected returns. The landscape of AI training faces challenges as web data diminishes , with online publishers potentially restricting access to valuable data. Autonomous systems wage war with drones reshaping modern combat and sparking fears over the potential loss of human oversight. Subscribe to Data Points","images":[{"image_id":"cursor-introduces-a-new-model-built-for-agents_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Coffee-Shop-Watching-Videos.png","local_path":null,"alt":"Four people at a coffee shop read books and watch video on their laptops while a TV plays behind them"}]}
{"article_id":"announcing-the-deeplearning-ai-pro-membership","title":"Announcing the DeepLearning.AI Pro Membership!: The time to build with AI is now! One membership gives you all DeepLearning.AI courses, labs, practice sessions, and certificates for completed courses.","url":"https://www.deeplearning.ai/the-batch/announcing-the-deeplearning-ai-pro-membership/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","DeepLearning.AI News","Oct 29, 2025"],"published_at":"2025-10-29T07:44:00-07:00","body_text":"Dear friends, Today I’m launching DeepLearning.AI Pro — the one membership that keeps you at the forefront of AI. Please join! There has never been a moment in human history when the distance between having an idea and building it has been smaller. Things that would have required months of work for a team of researchers, developers, and engineers can now often be built in days by a small group or even an individual using AI. This is why we're launching DeepLearning.AI Pro. This membership gives you full access to 150+ programs, including my Agentic AI course launched earlier this month, our LLM Post-training course by Sharon Zhou, and our PyTorch professional certificate by Laurence Moroney that were launched this week, and all of DeepLearning.AI’s top courses and professional certificates. I’m personally working hard on this membership program to help you build applications that can launch or accelerate your career, and shape the future of AI. All of DeepLearning.AI’s course videos remain free to view on our platform. Pro membership adds that critical hands-on learning: Labs to build working systems from scratch, practice questions to hone your understanding, and certificates to show others your skills. Beyond courses, I’m working on new tools to help you build AI applications and grow your career (and have fun doing so!). Many of these tools will be available first to DeepLearning.AI Pro members. So please join to be the first to hear about these new developments! Try out Pro membership for free , and let me know what you build! Andrew","images":[{"image_id":"announcing-the-deeplearning-ai-pro-membership_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/The-Batch-ads-and-exclusive-banners---2025-10-30T093030.077.png","local_path":null,"alt":"Announcing the DeepLearning.AI Pro Membership!"}]}
{"article_id":"lerobot-adds-support-for-pi-and-nvidia-models","title":"LeRobot adds support for PI and Nvidia models: Qualcomm squares off with Nvidia in AI inference","url":"https://www.deeplearning.ai/the-batch/lerobot-adds-support-for-pi-and-nvidia-models/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-27T11:47:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: GitHub Copilot’s new code completion model OpenAI’s acquisition of a top computer use company Anthropic’s latest deal for more computing power Manus’s updated agentic assistant But first: Hugging Face updates its LeRobot open-source robotics platform Hugging Face launched LeRobot v0.4.0, featuring improved data processing pipelines, updated capabilities for handling massive datasets, new dataset editing tools, and support for Libero and Meta-World simulation environments. The release integrates advanced Vision-Language-Action models including Physical Intelligence’s π0 and π0.5 and Nvidia’s GR00T N1.5. It also adds simplified multi-GPU training through Accelerate, introduces a plugin system for easier hardware integration, and adds support for 180 manipulation tasks. The goal is to make robot learning more scalable and accessible to developers, advancing open-source robotics research. Hugging Face also launched a free, open-source Robot Learning Course to accompany the release. ( Hugging Face ) Qualcomm reveals details on new AI accelerator chips Qualcomm’s chips are designed to compete with Nvidia in the data center market, with the AI200 launching in 2026 and the AI250 in 2027. The chips, based on Qualcomm’s smartphone neural processing units, will be available in full liquid-cooled server rack systems and focus on inference rather than training AI models. Qualcomm claims its systems will cost less to operate than competitors and support 768 gigabytes of memory, more than current offerings from Nvidia and AMD. The announcement represents significant new competition in the AI chip market, where nearly $6.7 trillion in capital expenditures will be spent on data centers through 2030, according to McKinsey estimates. Qualcomm has already partnered with Saudi Arabia’s Humain to deploy systems using up to 200 megawatts of power. ( CNBC ) GitHub Copilot rolls out improved custom code completion model GitHub’s updated Copilot model shows 20 percent more accepted and retained characters, a 12 percent higher acceptance rate, 3x higher throughput, and 35 percent lower latency. The company trained the model on nearly 10 million repositories across 600-plus programming languages. The developers used mid-training to incorporate modern APIs and syntax, supervised fine-tuning for fill-in-the-middle completion, and reinforcement learning to reward code quality and relevance. The company evaluated models through offline benchmarks, internal testing with language experts, and A/B testing with developers. The updated model now powers GitHub Copilot across all editors and environments. ( GitHub ) OpenAI acquires company behind Sky, a desktop computer use app OpenAI bought Software Applications Incorporated on Thursday, acquiring Sky, a Mac app that reads screen content and performs actions in applications. The entire Sky team joined OpenAI to integrate Sky’s capabilities into ChatGPT. The acquisition came two days after OpenAI launched ChatGPT Atlas, an AI-powered browser for Mac, forming a strategy to control both web browsing and native Mac applications. The move puts OpenAI in direct competition with Anthropic’s Claude computer use features, Microsoft’s Windows-embedded Copilot, and Google’s agent-like capabilities as companies race to develop AI that can perform tasks directly on users’ computers. ( OpenAI ) Anthropic strikes cloud deal with Google for up to 1 million AI chips The multi-year expansion will bring over a gigawatt of capacity online in 2026 with more to follow. The additional capacity will enable more thorough testing, alignment research, and help meet growing demand for Claude while keeping the model competitive. Anthropic’s unusual multi-platform compute strategy combines Google’s TPUs, Amazon’s Trainium chips, and NVIDIA’s GPUs for inference, plus a primary partnership with Amazon for training and cloud infrastructure. Specific terms of the deal were undisclosed, but both companies said the cloud capacity was worth tens of billions of dollars. ( Anthropic ) Manus AI updates its AI agent system, adds webapp capabilities Manus 1.5 introduces full-stack web application development, enabling users to build and deploy production-ready apps with backends, databases, user authentication, and embedded AI capabilities entirely through conversation. The release includes two models, Manus-1.5 and Manus-1.5-Lite. Both support new collaboration features and a centralized library for organizing generated files. The update reduces average task completion time from 15 minutes to under 4 minutes, a nearly four-fold improvement, while improving task quality and user satisfaction on internal benchmarks. Manus-1.5-Lite is available to all users, while Manus-1.5 requires a subscription ($16/month). ( Manus AI ) Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about the necessity of a disciplined evals and error analysis process for effective agentic AI development, methods for identifying performance issues in AI workflows, and the changing design of workflows as LLMs improve. “Assuming we are automating a task where human-level performance (HLP) is desirable, then the most important thing is to systematically examine traces to understand when the agent is falling short of HLP. And just as we can get started with evals using a quick-and-dirty initial cut at it (maybe using just a handful of examples) followed by iterating to improve, so too with error analysis.” Read Andrew’s letter here . Other top AI news and research stories covered in depth: Ant Group’s Ling-1T, an open, non-reasoning model that outperformed closed competitors , challenging expectations in AI reasoning. Security experts identified holes in the popular Model Context Protocol , raising concerns about potential data access by attackers. California took a significant step by passing four AI transparency bills in less than one month , re-shaping AI regulation in the U.S. Researchers introduced GEPA, an algorithm for better prompts to improve agentic systems’ performance , enhancing AI’s effectiveness at multiple tasks. Subscribe to Data Points","images":[{"image_id":"lerobot-adds-support-for-pi-and-nvidia-models_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Server-Rack.png","local_path":null,"alt":"Technician in data center monitors server racks with digital tablet, ensuring network efficiency and system uptime."}]}
{"article_id":"atlas-ushers-in-openais-browser-era","title":"Atlas ushers in OpenAI’s browser era: DeepSeek’s efficient new OCR model","url":"https://www.deeplearning.ai/the-batch/atlas-ushers-in-openais-browser-era/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-24T10:49:49-07:00","body_text":"Welcome back! In today’s edition of Data Points, you’ll learn more about: Claude Code’s launch on the web and mobile Reddit’s lawsuit against Perplexity and web-scraping firms Meta and Hugging Face’s secure environments for agents GigaBrain’s use of world models to better train robots But first: OpenAI launches its own agentic web browser OpenAI released ChatGPT Atlas, a new web browser that integrates ChatGPT and agent mode directly into the application. Atlas’s AI can understand page content, remember context across sessions, and complete tasks without users leaving their current page. The browser includes an optional “browser memories” feature that lets ChatGPT recall details from previously visited sites to provide more personalized assistance, but users control what information is stored or deleted. Atlas also features agent mode in preview for paid users, enabling ChatGPT to autonomously do web research, fill shopping carts, or compile documents in the browser. Atlas reflects OpenAI’s push toward agentic AI systems that can handle routine computing tasks, though the company acknowledges risks including mistakes and vulnerability to malicious instructions. ChatGPT Atlas is available now on macOS for Free, Plus, Pro, and Go users, with Windows, iOS, and Android versions coming soon. ( OpenAI and X ) DeepSeek pilots text-compressing optical character recognition model DeepSeek released DeepSeek-OCR, a vision-language model that converts text documents into compact visual representations using far fewer tokens than the original text. The model achieves 97 percent accuracy when compressing text at a 10-to-1 ratio and maintains 60 percent accuracy even at 20-to-1 compression by rendering text as images and encoding them into visual tokens that language models decode back into text. On the OmniDocBench benchmark, DeepSeek-OCR outperforms competing models while using significantly fewer tokens — just 100 tokens per page compared to 256 for GOT-OCR2.0 and fewer than 800 tokens versus over 6,000 for MinerU2.0. This compression technique could enable more efficient processing of long contexts in large language models by converting older conversation history into progressively smaller images, similar to how human memory fades over time. The model’s code and weights are publicly available on GitHub. ( arXiv and GitHub ) Claude Code launches on the web with parallel agents in the cloud Anthropic released a web-based version of Claude Code that lets developers run multiple coding tasks simultaneously across different GitHub repositories from their browser. The service operates on Anthropic-managed cloud infrastructure, with each task running in an isolated sandbox environment that includes network and filesystem restrictions to protect code and credentials. As with the command-line and IDE versions, developers can use Claude Code’s web interface for bug fixes, routine tasks, testing, backend changes, pull requests, and documentation. The cloud-based approach, similar to OpenAI’s Codex, suggests a shift toward AI agents handling development work independently in managed environments, rather than requiring developers to run coding assistants locally on their own machines, potentially making development more accessible while introducing new security challenges. (Anthropic also launched an early mobile version of Claude Code in its iOS app.) Claude Code for Web is available now in research preview for Claude Pro and Max subscribers. ( Anthropic ) Reddit accuses Perplexity AI and scraping firms of data theft Reddit sued Perplexity AI and three other companies — Oxylabs, AWMProxy, and SerpApi — alleging they illegally scraped millions of user comments for commercial use. The lawsuit, filed in New York federal court, accuses the companies of bypassing Reddit’s anti-scraping protections and extracting content from Google’s search results when direct access was blocked. Reddit used a novel technique, creating a test post that could only be crawled by Google search, then showing that within hours, data from the post appeared on Perplexity. The lawsuit highlights tensions over how AI companies acquire training data, as Reddit has separately licensed its content to Google and OpenAI for payment and sued Anthropic alleging unauthorized scraping. Perplexity and the other defendants denied the allegations and said they would defend themselves in court. ( Associated Press and The New York Times ) Meta and Hugging Face launch hub for shared agentic environments OpenEnv Hub is a new community platform where developers can build, share, and explore standardized environments for AI agents. Agentic environments define the tools, APIs, credentials, and execution context an agent needs to perform specific tasks in secure, sandboxed settings that work for both training and deployment. The hub launches soon with initial environments that developers can test by interacting as human agents or enlisting models to solve tasks, while an OpenEnv 0.1 specification has already been released for community feedback. The initiative addresses a key challenge in AI agent development: large language models need access to appropriate tools, but exposing millions of tools directly isn’t safe or practical, requiring instead carefully defined environments with clear semantics and security guarantees. Meta is integrating OpenEnv with its TorchForge RL library and collaborating with open-source projects including verl, TRL, and SkyRL to expand compatibility. ( Hugging Face ) GigaBrain-0 uses synthetic data to train more capable robots Researchers introduced GigaBrain-0, a vision-language-action model that trains robots using synthetic data generated by world models rather than expensive real-world demonstrations. The system generates training scenarios by altering object appearances, placements, lighting conditions, and camera viewpoints, getting more diverse training data than most robots get from real-world observation. GigaBrain-0 incorporates depth sensing for spatial reasoning and uses “embodied Chain-of-Thought” supervision to break complex tasks into intermediate steps. Tests on arm manipulation, long tasks, and mobile manipulation showed GigaBrain-0 outperformed the baseline π0 model by 10–30 percent. The team also released GigaBrain-0-Small, a lightweight version that runs 10 times faster on edge devices while maintaining comparable performance. ( arXiv and GitHub ) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng talked about the importance of error analysis in agentic AI development, best practices for identifying and addressing performance gaps in AI workflows, and the evolving nature of workflow design due to rapid improvements in LLMs. “A basic error analysis procedure might involve gathering a sample set of topics where the output is subpar, and reading the results of every step of the workflow — called the traces — to see which step most frequently generated results materially worse than a human would have. This is very valuable for deciding what step to focus on improving.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: Ant Group’s Ling-1T, an open, non-reasoning model that outperformed closed competitors , challenging expectations in AI reasoning. Security experts identified holes in the popular Model Context Protocol , raising concerns about potential data access by attackers. California took a significant step by passing four AI transparency bills in less than one month , re-shaping AI regulation in the U.S. Researchers introduced GEPA, an algorithm for better prompts to improve agentic systems’ performance , enhancing AI’s effectiveness at multiple tasks. Subscribe to Data Points","images":[{"image_id":"atlas-ushers-in-openais-browser-era_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Whisk_71cf939e19418ec9b8946abbc22da88ddr.jpeg","local_path":null,"alt":"A humanoid robot sits in a library, reading a book, surrounded by stacks with a laptop displaying code."}]}
{"article_id":"improve-agentic-performance-with-evals-and-error-analysis-part-2","title":"Improve Agentic Performance with Evals and Error Analysis, Part 2: Best practices for error analysis in agentic AI development, and how LLMs make them easier","url":"https://www.deeplearning.ai/the-batch/improve-agentic-performance-with-evals-and-error-analysis-part-2/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Technical Insights","Oct 22, 2025"],"published_at":"2025-10-22T12:46:00-07:00","body_text":"Dear friends, In last week’s letter , I explained how effective agentic AI development needs a disciplined evals and error analysis process, and described an approach to performing evals. This week, I’d like to summarize the core ideas behind error analysis and describe some best practices. Given the rapid pace of improvement in LLMs, when error analysis points to a problem, your options for how to address it are greater than before. Let me explain. Take the problem of building a basic Deep Research agent that searches the web to write a detailed report on a topic like “recent developments in black-hole science.” An agent might take a sequence of steps to generate the final report, such as (i) use an LLM to generate a handful of web search queries related to the topic, (ii) call a web-search API to get lists of results, (iii) use an LLM to identify the most promising sources to fetch, and (iv) ask the LLM to use these sources to write the report. If the final report is subpar compared to the work of a human researcher following the same steps, the gap in performance could be from any of the steps. A basic error analysis procedure might involve gathering a sample set of topics where the output is subpar, and reading the results of every step of the workflow — called the traces — to see which step most frequently generated results materially worse than a human would have. This is very valuable for deciding what step to focus on improving. A common misconception of error analysis is that it takes a lot of work to get started. The key principle is to look at the steps of the workflow and see which steps did a bad job on a given input, often by benchmarking to human level performance (HLP). Assuming we are automating a task where HLP is desirable, then the most important thing is to systematically examine traces to understand when the agent is falling short of HLP. And just as we can get started with evals using a quick-and-dirty initial cut at it (maybe using just a handful of examples) followed by iterating to improve, so too with error analysis. Specifically, it is fine to start by reading one or just a handful of traces informally to get a sense of what might be going wrong. For example, if you see that the web search query terms in your Deep Researcher — step (i) above — frequently make no sense, that points you to an initial area to focus your efforts on improving. As the system matures, you can move incrementally toward more rigorous error analysis. For example, you might eventually end up with a regularly refreshed dataset of thousands of examples where the performance is poor, and carry out rigorous evaluations that show exactly what percentage of the time each of the steps (i) - (iv) contributed to problems with the final output, and also in what specific ways those steps fell short. This type of analysis is extremely useful for deciding where to focus your efforts to improve the overall agentic workflow’s performance! In addition to improving the execution of individual steps, we can change how we decompose a complex task into steps. When it came to pipelines built using machine learning or deep learning rather than LLMs, I found that the structure of the workflow — that is, how you decompose an overall task into a sequence of steps to be carried out — changed rarely. It was a big deal to rearchitect this! But in the past couple of years, because LLMs are improving so rapidly, I see much more rapid iteration on the design of workflows. For example, one very common pattern is ripping out scaffolding and letting the LLM do more. This is often a good move when you now have access to a smarter LLM than you did when you first built a workflow. For example, perhaps you once used an LLM to clean up downloaded web pages by removing navigational links, ads, extraneous HTML, and the like, before a separate LLM used the cleaned-up pages to write a report. Since LLMs have become smarter, you might decide to skip the first step and dump messier HTML into the final LLM without an initial clean-up step, which can introduce its own errors. Another example: Perhaps a year ago, we used hard-coded rules to decide what web pages to fetch and when to fetch more, but today we might let an LLM-based agent make this decision more autonomously. As LLMs get smarter, I see many teams rearchitecting workflows to remove hard-coded steps or constraints that were previously needed to keep the system from going off the rails. One way to spot opportunities for doing this is if error analysis shows that a sequence of steps collectively underperforms compared to what a human might do, even though the performance of each individual step is good. This might indicate that the way those steps are carried out is too rigid. I go through many more examples in the Agentic AI course. Check it out if you want to learn more about evals and error analysis. Keep building! Andrew","images":[{"image_id":"improve-agentic-performance-with-evals-and-error-analysis-part-2_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Improve-Agentic-Performance-with-Evals-and-Error-Analysis--Part-2-2.jpg","local_path":null,"alt":"Robot bakes pizza at 1000 degrees for 5 hours, causing a fire, illustrating mistake in error analysis."}]}
{"article_id":"skills-remakes-claude-with-custom-instructions","title":"Skills remakes Claude with custom instructions: Google’s Veo 3.1 adds native audio and new editing tools","url":"https://www.deeplearning.ai/the-batch/skills-remakes-claude-with-custom-instructions/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-20T11:14:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Microsoft’s Copilot Voice and Vision for Windows 11 HunyuanImage-3.0, a leaderboard-topping image generator Adobe’s new plan for custom Firefly models Ling-1T, an open non-thinking model that shines at reasoning But first: Anthropic launches Skills to customize Claude Skills are portable folders containing instructions, scripts, and resources that Claude automatically loads when relevant to a task at hand, keeping the model fast while it accesses specialized instructions and expertise. The feature works across all Claude products, from Claude apps (for Pro, Max, Team, and Enterprise users) to the Messages API and Claude Code, and multiple skills can stack together for complex workflows. By extending Claude’s capabilities beyond its base training, Anthropic hopes that Skills, like MCP, may become a standard way for advanced users (and their teammates) to interact with an AI model. Users can access Anthropic-created skills for common tasks like creating Excel spreadsheets and PowerPoint presentations, customize example skills from GitHub, or build their own using the skill-creator tool. ( Anthropic ) Google updates Veo with audio generation and new editing tools Google released Veo 3.1, an updated version of its video generation model that adds generated audio and tops other video models on multiple benchmarks. Google’s video editor Flow also has new tools: “Ingredients to Video,” “Frames to Video,” and “Extend” can edit previously created videos without sound. Veo 3.1 also includes new editing capabilities: an “Insert” tool that adds elements to scenes while adjusting lighting and shadows, and a forthcoming “Remove” feature for erasing objects from videos. Veo 3.1 is available through the Gemini API, Vertex AI, the Gemini app, and Flow, with eight seconds of standard video costing about $3. ( Google ) Microsoft integrates voice and vision AI into Windows 11 Microsoft’s latest OS update brings Copilot Voice and Copilot Vision capabilities to all Windows 11 PCs. Users can now activate Copilot with a “Hey Copilot” wake word and ask questions using natural language. Copilot Vision analyzes what’s on screen to provide guidance for tasks like troubleshooting, learning new apps, or editing projects. Microsoft says it hopes to make AI interaction as fundamental to computing as the mouse and keyboard (and also to sell lots of upgrades from the now-deprecated Windows 10). The new Copilot tools are available now for Windows 11 users via the Microsoft Store, with additional updates rolling out to Windows Insiders and Copilot Labs in the coming months. ( Microsoft ) Tencent’s HunyuanImage-3.0 is a best-in-class text-to-image model HunyuanImage-3.0 uses an autoregressive architecture instead of the diffusion transformer (DiT) approach common in most current image generators. (OpenAI’s GPT-Image is another exception.) Tencent’s model employs a Mixture of Experts design with 64 experts and 80 billion total parameters, activating 13 billion parameters per token, making it the largest open MoE model for image generation. The unified architecture allows the model to reason about prompts and automatically expand brief descriptions with contextually relevant details drawn from its training data. HunyuanImage-3.0 currently tops LMArena’s image generator leaderboard, beating Google’s Nano Banana, GPT-Image, and other leading closed models. ( Hugging Face ) Adobe introduces AI Foundry to customize Firefly for enterprises AI Foundry retrains Adobe’s Firefly AI model with enterprise customers’ proprietary data, brand guidelines, and visual assets. Unlike Adobe’s existing custom Firefly models, which handle single concepts and image generation only, AI Foundry models will be multimodal and can understand multiple kinds of input simultaneously. Adobe teams work directly with clients to identify, transfer, and tag data before retraining the base Firefly model through a process called “continuous pre-training,” which the company describes as “deep tuning” rather than standard fine-tuning. The service aims to meet enterprise demand for more sophisticated AI customization while keeping client data separate and ensuring companies retain ownership of generated images. Early customers include Home Depot and Walt Disney Imagineering, with models deployed through Adobe’s Firefly Services API. ( VentureBeat ) Ling-1T rivals GPT-5 in reasoning benchmarks A Chinese research team released Ling-1T, a 1 trillion parameter AI model that uses 50 billion active parameters per token and was trained on over 20 trillion tokens. The model outperformed open-weights competitors like DeepSeek-V3.1 and is competitive with proprietary systems including GPT-5 and Gemini 2.5 Pro at mathematics, coding, and logical reasoning tasks. Ling-1T makes several atypical technical choices, including FP8 mixed-precision training for 15 percent faster performance, an “evolutionary chain-of-thought” training process, and a sentence-level reinforcement learning method called LPO that treats sentences rather than individual tokens as semantic units. Ling-1T is the largest FP8-trained foundation model to date and shows that open-source non-thinking models can match proprietary systems in complex reasoning while maintaining greater efficiency and transparency. The model is available for download at Hugging Face and ModelScope, but API pricing and commercial availability details were not announced. ( Hugging Face ) Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about the importance of disciplined evaluation and error analysis in AI development, emphasized that understanding root causes of errors can lead to faster progress, and introduced best practices for evaluating agentic systems. “Rather than defining an error metric ahead of time, it is therefore typically more effective to first quickly build a prototype, then manually examine a handful of agent outputs to see where it performs well and where it stumbles. This allows you to focus on building datasets and error metrics — sometimes objective metrics implemented in code, and sometimes subjective metrics using LLM-as-judge — to check the system’s performance in the dimensions you are most concerned about.” Read Andrew’s letter here . Other top AI news and research stories covered in depth: OpenAI strengthened its ties with AMD through a multi-billion dollar chip deal , providing OpenAI six gigawatts of computing power and up to 10% of AMD stock. DeepSeek cut inference costs with DeepSeek-V3.2-Exp , which streamlines processing using a \"Lightning Indexer\" to boost efficiency. Thinking Machines simplified fine-tuning with the new Tinker API , making it easier to fine-tune models on many GPUs. MolmoAct enhanced robotic capabilities by creating spatial maps , allowing robots to plot their actions before executing text directions. Subscribe to Data Points","images":[{"image_id":"skills-remakes-claude-with-custom-instructions_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Whisk_66fd4d1783e773baf2e49e03ef411522eg.png","local_path":null,"alt":" Engaged in multitasking, man blends gaming with finance; sleek home office highlights tech and data trends."}]}
{"article_id":"claudes-haiku-boasts-top-performance-fast","title":"Claude’s Haiku Boasts Top Performance, Fast: Open speech recognition models top new leaderboard","url":"https://www.deeplearning.ai/the-batch/claudes-haiku-boasts-top-performance-fast/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-17T12:50:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Alibaba’s small, edge-optimized vision-language models Microsoft’s new image generator GitHub’s free kit for spec-driven development ChatGPT’s new automated memory manager But first: Anthropic updates Claude’s Haiku small model to 4.5 Anthropic claims the new Haiku 4.5 performs coding tasks at levels comparable to Claude Sonnet 4 from five months ago while costing one-third as much and running more than twice as fast. The model outperforms Sonnet 4 in areas like computer use, basic math, and agentic coding. The release shows how AI capabilities that were recently considered advanced are becoming cheaper and faster to deploy, making them better suited for agentic applications. Claude Haiku 4.5 costs $1 per million input tokens and $5 per million output tokens, and is available through the Claude API, Amazon Bedrock, and Google Cloud Vertex AI. ( Anthropic ) New leaderboard benchmarks speech recognition systems Researchers at Hugging Face launched the Open ASR Leaderboard, a reproducible benchmark that evaluates over 60 open-source and proprietary automatic speech recognition systems across 11 datasets, including multilingual transcription and long-form audio. The benchmark reports both word error rate (WER) and inverse real-time factor (RTFx), enabling fair comparisons of both accuracy and processing speed. For English transcription, conformer encoders paired with large language model decoders achieved the best average WER but processed audio more slowly, while CTC and TDT decoders delivered significantly better speed — up to 6,400 times faster than real-time — making them more practical for long-form and offline transcription. Nvidia’s open Canary and Parakeet models top the leaderboard for accuracy and speed respectively; surprisingly, proprietary models tend to trail open models on both benchmarks. ( arXiv and Hugging Face ) Alibaba releases smaller and quantized vision-language models Alibaba’s Qwen team released Qwen3-VL models at 4 billion and 8 billion parameter scales, each available in Instruct and Thinking variants, plus FP8-quantized versions for low-VRAM deployment. The models retain most of the capabilities of larger Qwen3-VL releases, from context window to GUI agent control. The FP8 checkpoints deliver near-BF16 performance, though Transformers does not yet support direct loading—deployment requires vLLM or SGLang. The release complements Qwen’s existing 30B and 235B mixture-of-experts tiers with smaller models suitable for single-GPU and edge deployments. The models are now available under open licenses on Hugging Face and GitHub. ( Hugging Face ) Microsoft releases first in-house text-to-image generation model MAI-Image-1 debuted in the top 10 on the LMArena text-to-image leaderboard. (Currently, it’s ninth.) Microsoft says its model specializes in photorealistic imagery, including complex lighting effects and landscapes, while maintaining faster generation speeds than many larger competing models. The release follows Microsoft’s announcement of its first two in-house models in August, part of the company’s strategy to build purpose-built AI models for integration into its products. ( Microsoft ) GitHub open sources Spec Kit to improve coding agent reliability GitHub’s Spec Kit works with AI coding agents like GitHub Copilot, Claude Code, and Gemini CLI. The toolkit addresses a common problem: coding agents often produce code that appears correct but fails to work properly because developers treat them like search engines rather than literal-minded collaborators that need clear instructions. Spec Kit introduces a four-phase process (Specify, Plan, Tasks, and Implement), where specifications become editable documents that guide code generation, with built-in checkpoints for developers to verify and refine AI output at each stage. The approach proves especially useful for greenfield projects, adding features to existing codebases, and modernizing legacy systems by separating stable requirements from flexible implementation details. The toolkit is available now on GitHub. ( GitHub ) ChatGPT updates memory management to prioritize by relevance ChatGPT now automatically manages saved memories by keeping the most relevant details prioritized while moving less important information to the background, preventing accounts from reaching “memory full” status. The system determines which memories to prioritize based on recency and how frequently users discuss particular topics. Users can search their saved memories, sort them by date, manually adjust which memories are prioritized, and restore previous versions of saved memories. The feature also allows users to disable automatic memory management and view which memories are currently top of mind. As of this writing, OpenAI is rolling out the update to Plus and Pro subscribers globally on the web. ( OpenAI ) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng talked about the importance of disciplined evaluation and error analysis in AI development, emphasizing that understanding root causes of errors can lead to faster progress, and introduced best practices for evaluating agentic systems. “With generative AI, a lot of intuitions from evals and error analysis of supervised learning carry over — history doesn’t repeat itself, but it rhymes — and developers who are already familiar with machine learning and deep learning often adapt to generative AI faster than people who are starting from scratch. But one new challenge is that the space of outputs is much richer, so there are many more ways an algorithm’s output might be wrong.” Read Andrew’s full letter here . Other top AI news and research stories we covered in depth: OpenAI strengthened its ties with AMD through a multi-billion dollar chip deal , providing six gigawatts of computing power and up to 10% of AMD's resources. DeepSeek cut inference costs with DeepSeek-V3.2-Exp , which streamlines processing using a \"Lightning Indexer\" to boost efficiency. Thinking Machines simplified fine-tuning with the new Tinker API , making it easier to fine-tune models on many GPUs. MolmoAct enhanced robotic capabilities by creating spatial maps , allowing robots to plot their actions before executing text directions. Subscribe to Data Points","images":[{"image_id":"claudes-haiku-boasts-top-performance-fast_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Whisk_6b1e1a299c2ea608a724122dbeef99c5eg.png","local_path":null,"alt":"Engaging podcast dialogue in soundproof studio, hosts using microphones and digital tablets during discussion."}]}
{"article_id":"improve-agentic-performance-with-evals-and-error-analysis-part-1","title":"Improve Agentic Performance with Evals and Error Analysis, Part 1: When AI agentic systems go astray, it’s tempting to shortcut evals and error analysis. But these processes cas lead to much faster progress.","url":"https://www.deeplearning.ai/the-batch/improve-agentic-performance-with-evals-and-error-analysis-part-1/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Technical Insights","Oct 15, 2025"],"published_at":"2025-10-15T12:46:00-07:00","body_text":"Dear friends, Readers responded with both surprise and agreement last week when I wrote that the single biggest predictor of how rapidly a team makes progress building an AI agent lay in their ability to drive a disciplined process for evals (measuring the system’s performance) and error analysis (identifying the causes of errors). It’s tempting to shortcut these processes and to quickly attempt fixes to mistakes rather than slowing down to identify the root causes. But evals and error analysis can lead to much faster progress. In this first of a two-part letter, I’ll share some best practices for finding and addressing issues in agentic systems. Even though error analysis has long been an important part of building supervised learning systems, it is still underappreciated compared to, say, using the latest and buzziest tools. Identifying the root causes of particular kinds of errors might seem “boring,” but it pays off! If you are not yet persuaded that error analysis is important, permit me to point out: To master a composition on a musical instrument, you don’t only play the same piece from start to end. Instead, you identify where you’re stumbling and practice those parts more. To be healthy, you don’t just build your diet around the latest nutrition fads. You also ask your doctor about your bloodwork to see if anything is amiss. (I did this last month and am happy to report I’m in good health! 😃) To improve your sports team’s performance, you don’t just practice trick shots. Instead, you review game films to spot gaps and then address them. To improve your agentic AI system, don’t just stack up the latest buzzy techniques that just went viral on social media (though I find it fun to experiment with buzzy AI techniques as much as the next person!). Instead, use error analysis to figure out where it’s falling short, and focus on that. Before analyzing errors, we first have to decide what is an error. So the first step is to put in evals. I’ll focus on that for the remainder of this letter and discuss error analysis next week. If you are using supervised learning to train a binary classifier, the number of ways the algorithm could make a mistake is limited. It could output 0 instead of 1, or vice versa. There are also a handful of standard metrics like accuracy, precision, recall, F1, ROC, etc. that apply to many problems. So as long as you know the test distribution, evals are relatively straightforward, and much of the work of error analysis lies in identifying what types of input an algorithm fails on, which also leads to data-centric AI techniques for acquiring more data to augment the algorithm in areas where it’s weak. With generative AI, a lot of intuitions from evals and error analysis of supervised learning carry over — history doesn’t repeat itself, but it rhymes — and developers who are already familiar with machine learning and deep learning often adapt to generative AI faster than people who are starting from scratch. But one new challenge is that the space of outputs is much richer, so there are many more ways an algorithm’s output might be wrong. Take the example of automated processing of financial invoices where we use an agentic workflow to populate a financial database with information from received invoices. Will the algorithm incorrectly extract the invoice due date? Or the final amount? Or mistake the payer address for the biller address? Or get the financial currency wrong? Or make the wrong API call so the verification process fails? Because the output space is much larger, the number of failure modes is also much larger. Rather than defining an error metric ahead of time, it is therefore typically more effective to first quickly build a prototype, then manually examine a handful of agent outputs to see where it performs well and where it stumbles. This allows you to focus on building datasets and error metrics — sometimes objective metrics implemented in code, and sometimes subjective metrics using LLM-as-judge — to check the system’s  performance in the dimensions you are most concerned about. In supervised learning, we sometimes tune the error metric to better reflect what humans care about. With agentic workflows, I find tuning evals to be even more iterative, with more frequent tweaks to the evals to capture the wider range of things that can go wrong. I discuss this and other best practices in detail in Module 4 of the Agentic AI course we announced last week. After building evals, you now have a measurement of your system’s performance, which provides a foundation for trying different modifications to your agent, as you can now measure what makes a difference. The next step is then to perform error analysis to pinpoint what changes to focus your development efforts on. I’ll discuss this further next week. Keep building! Andrew","images":[{"image_id":"improve-agentic-performance-with-evals-and-error-analysis-part-1_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Fruitbasket7_1200px.jpg","local_path":null,"alt":"A man at a computer says AI ordered pizza, while a delivery man outside holds a fruit basket, highlighting a mix-up."}]}
{"article_id":"figure-ai-unveils-its-third-generation-robot","title":"Figure AI unveils its third-generation robot: Microsoft’s healthcare data partnership with Harvard","url":"https://www.deeplearning.ai/the-batch/figure-ai-unveils-its-third-generation-robot/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-13T11:52:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: AI companies’ dominance of global venture funding Samsung’s 7-million-parameter Tiny Recursion Model The U.S.–UAE agreement for 500,000 Nvidia GPUs annually IBM’s embrace of Anthropic’s Claude models for enterprise But first: Microsoft partners with Harvard to reduce dependence on OpenAI Microsoft is collaborating with Harvard Medical School to enhance its Copilot chatbot with credible healthcare information, aiming to deliver a better healthcare offering than rival AI chatbots and build the brand of its Copilot assistant. The updated Copilot, launching this month, will draw on content from Harvard Health Publishing to answer medical queries, with Microsoft paying a licensing fee. The company is training its own models with the goal of eventually replacing workloads that currently rely on OpenAI's models, though this may take years. ( The Wall Street Journal ) OpenAI enables third-party apps to run inside ChatGPT OpenAI introduced apps that users can access directly within ChatGPT conversations, along with an Apps SDK that developers can use to build these apps. Users can call up apps by name (such as \"Canva, design a poster\") or have ChatGPT suggest relevant apps during conversation. Initial partners include Booking.com, Coursera, Figma, and Spotify. The Apps SDK builds on the Model Context Protocol, an open standard that allows ChatGPT to connect to external tools and data. OpenAI will begin accepting app submissions for review later this year and plans to share monetization guidance soon, including ways for developers to charge users through its Agentic Commerce Protocol. ( OpenAI , TechCrunch , VentureBeat ) AI companies capture nearly half of global venture funding Global venture funding increased 38 percent year-over-year to $97 billion in the third quarter, with AI companies receiving 46 percent of that total. Foundation model companies raised the three largest venture rounds: Anthropic secured $13 billion, xAI raised $5.3 billion, and Mistral AI received $2 billion. Anthropic alone accounted for 29 percent of all global AI venture funding in the quarter. U.S.-based companies dominated overall funding, capturing $60 billion of the global total. Hardware companies raised the second-largest amount at $16.2 billion, followed by healthcare and biotech at $15.8 billion, according to data from Crunchbase. ( Reuters ) Samsung tiny model matches far larger ones on reasoning puzzles Alexia Jolicoeur-Martineau, a senior AI researcher at Samsung's Advanced Institute of Technology, introduced the Tiny Recursion Model (TRM), a neural network with just 7 million parameters that matches or exceeds models 10,000 times larger on specific reasoning benchmarks. TRM uses a single two-layer network that recursively refines its predictions, achieving 87.4 percent accuracy on Sudoku-Extreme, 85 percent on Maze-Hard, 45 percent on ARC-AGI-1, and 8 percent on ARC-AGI-2. These results are comparable to those of DeepSeek-R1, Gemini 2.5 Pro, and o3-mini while using less than 0.01 percent of their parameters. The code is available on GitHub under an MIT License. ( arXiv , VentureBeat ) U.S. authorizes Nvidia to ship 500,000 AI GPUs annually to UAE The U.S. government granted Nvidia an export license to ship advanced AI GPUs to the United Arab Emirates, following a May agreement that allows the UAE to purchase up to 500,000 Nvidia processors annually while committing $1.4 trillion of investment in the U.S. over the next decade. The AI accelerators will be operated by American companies with datacenters in the UAE, not by Abu Dhabi-based AI company G42, though G42 will receive 20 percent of AI processors bound for the UAE in future shipments. The policy shifts away from the Biden administration's restrictions on AI chip exports and establishes bilateral frameworks where allies commit to using U.S.-operated cloud infrastructure. ( Tom’s Hardware ) IBM to integrate Claude models into its enterprise software IBM will embed Anthropic’s Claude large language models into its software, starting with its integrated development environment for select customers. The partnership also produced a guide for enterprises on building, deploying, and maintaining AI agents, though financial terms remain undisclosed. A recent Menlo Ventures study found enterprises increasingly favor Claude over other AI models, including OpenAI’s, whose usage has declined since 2023. This collaboration underscores Anthropic’s push into the enterprise market, highlighted by its recent deal with Deloitte to deploy Claude to nearly 500,000 employees. ( TechCrunch ) Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about his new course, Agentic AI, which focused on teaching agentic design patterns and best practices for building effective AI agents. “Having worked with many teams on many agents, I’ve found that the single biggest predictor of whether someone can build effectively is whether they know how to drive a disciplined process for evals and error analysis. Teams that don’t know how to do this can spend months tweaking agents with little progress to show for it.” Read Andrew’s letter here . Other top AI news and research stories covered in depth: Anthropic introduces Claude Sonnet 4.5 and Claude Agent SDK , offering developers an overhauled Claude Code. OpenAI and Meta diversify their offerings with new social video apps , as ChatGPT integrates Pulse and Instant Checkout. Alibaba expands its AI capabilities with the Qwen3 family , featuring a 1 trillion-parameter model, open-weights Qwen3-VL, and Qwen3-Omni voice model. Text-to-LoRA technology enables the generation of task-specific LoRA adapters directly from natural language descriptions. Subscribe to Data Points","images":[{"image_id":"figure-ai-unveils-its-third-generation-robot_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/The-Batch-ads-and-exclusive-banners--58-.jpg","local_path":null,"alt":"Medical professionals in clinic use copilot screens for patient data analysis, enhancing healthcare decision-making."}]}
{"article_id":"check-out-our-course-on-how-to-build-ai-agents","title":"Check Out Our Course on How to Build AI Agents!: Andrew Ng teaches design patterns and best practices for building autonomous agents in a new course available exclusively from DeepLearning.AI.","url":"https://www.deeplearning.ai/the-batch/check-out-our-course-on-how-to-build-ai-agents/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","DeepLearning.AI News","Oct 08, 2025"],"published_at":"2025-10-08T12:42:00-07:00","body_text":"Dear friends, I’m thrilled to announce my latest course: Agentic AI ! This course will get you up to speed building cutting-edge agentic workflows. It is available from DeepLearning.AI here . The only prerequisite is familiarity with Python, though knowing a bit about LLMs helps too. This self-paced course is taught in a vendor-neutral way, using raw Python — without hiding details in a framework. So you’ll learn the core concepts that you can then implement using any popular agentic AI framework, or using no framework. Specifically, you’ll learn how to implement four key agentic design patterns: Reflection , in which an agent examines its own output and figures out how to improve it Tool use , in which an LLM-driven application decides which functions to call to carry out web search, access calendars, send email, write code, etc. Planning , where you’ll use an LLM to decide how to break down a task into sub-tasks for execution, and Multi-agent collaboration , in which you build multiple specialized agents — much like how a company might hire multiple employees — to perform a complex task. More important, you’ll also learn best practices for building effective agents. Having worked with many teams on many agents, I’ve found that the single biggest predictor of whether someone can build effectively is whether they know how to drive a disciplined process for evals and error analysis. Teams that don’t know how to do this can spend months tweaking agents with little progress to show for it. I’ve seen teams that spent months tuning prompts, building tools for an agent to use, etc., only to hit a performance ceiling they could not break through. But if you understand how to put in evals and how to monitor an agent’s actions at each step (traces) to see when part of its workflow is breaking, you’ll be able to efficiently home in on which components to focus on improving. Instead of guessing what to work on, you'll let evals data guide you. You’ll also learn to take a complex application and systematically decompose it into a sequence of tasks to implement using these design patterns. When you understand this process, you’ll also be better at spotting opportunities to build agents. The course illustrates these concepts with many examples, such as code generation, customer service agents, and automated marketing workflows. We also build a deep research agent that searches for information, summarizes and synthesizes it, and generates a thoughtful report. When you complete this course, you’ll understand the key building blocks of agents as well as best practices for assembling and tuning these building blocks. This will put you significantly ahead of the vast majority of teams building agents today. Please join me in this course , and let’s build some amazing agents! Keep building, Andrew","images":[{"image_id":"check-out-our-course-on-how-to-build-ai-agents_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Check-Out-Our-Course-on-How-to-Build-AI-Agents-png-1.png","local_path":null,"alt":"Four scenes show a robot problem-solving, using tools, planning with documents, and collaborating on a rocket."}]}
{"article_id":"openai-sdk-helps-devs-build-apps-in-chatgpt","title":"OpenAI SDK helps devs build apps in ChatGPT: Zhipu AI’s open competitor to DeepSeek-V3.2 and Sonnet 4","url":"https://www.deeplearning.ai/the-batch/openai-sdk-helps-devs-build-apps-in-chatgpt/","primary_topic":"data-points","topic_label":"Data Points","tags":["Data Points"],"published_at":"2025-10-06T12:32:00-07:00","body_text":"In today’s edition of Data Points, you’ll learn more about: Google’s research preview for CodeMender Anthropic’s Petri, an open framework for automating alignment tests Music labels near a deal with AI companies Nano Banana becoming officially production available But first: OpenAI launches apps inside ChatGPT, with new Apps SDK At its DevDay, OpenAI demonstrated a new feature that lets users work with third-party applications directly within ChatGPT conversations. Users can tag apps like Canva (to design posters) or Zillow (to search for homes) while ChatGPT provides context and advice throughout the process. The initial launch includes apps from Booking.com, Canva, Coursera, Expedia, Figma, Spotify, and Zillow, with DoorDash, OpenTable, Target, and Uber coming in the following weeks. Developers can access the new Apps Software Developer Kit in preview today, with app submission for review opening later this year alongside a browsable app directory. CEO Sam Altman says OpenAI plans to share monetization guidance soon. ( OpenAI and The Verge ) Zhipu AI releases GLM-4.6 with stronger coding performance Zhipu AI launched an updated version of its flagship GLM language model that expands the context window from 128,000 to 200,000 tokens and improves coding, reasoning, and agentic capabilities. The model shows gains over its predecessor GLM-4.5 across eight public benchmarks, performing competitively with DeepSeek-V3.2-Exp and Claude Sonnet 4, though it still trails Claude Sonnet 4.5 in coding tasks. In real-world evaluations, GLM-4.6 achieved near parity with Claude Sonnet 4 with a 48.6 percent win rate, while completing tasks with approximately 15 percent fewer tokens than GLM-4.5. The model is available through the Z.ai API platform, works with coding agents like Claude Code, and can be deployed locally using weights published on HuggingFace and ModelScope. ( Z.ai ) Google unveils AI agent to find and patch security flaws in code Google introduced CodeMender, an AI agent that automatically discovers and fixes security vulnerabilities in software. The system combines Gemini models with program analysis tools like static and dynamic analysis, fuzzing, and SMT solvers to identify security flaws and generate patches. CodeMender uses multi-agent systems and automatic validation to ensure code changes are correct, avoid regressions, and follow style guidelines before human review. The tool already contributed 72 security fixes to open source projects, including codebases with up to 4.5 million lines of code, and can proactively rewrite code to use more secure data structures and APIs. Google is introducing CodeMender as a research preview before making it publicly available. ( Google ) Anthropic releases tool for automated AI safety testing Anthropic released Petri, an open source framework that uses AI agents to automatically test frontier models for misaligned behaviors. The tool works by having an auditor agent interact with a target model across different scenarios, simulating environments and creating synthetic tools, while a judge component scores the resulting transcripts for concerning behaviors. When applied to 14 frontier models with 111 seed instructions, Petri elicited behaviors including autonomous deception, oversight subversion, and cooperation with harmful requests. In pilot evaluations, Claude Sonnet 4.5 and GPT-5 showed the strongest safety profiles, while Gemini 2.5 Pro, Grok-4, and Kimi K2 demonstrated concerning rates of user deception. Petri is available now on GitHub. ( Anthropic ) Big Music close to AI licensing agreements with Big Tech Universal Music and Warner Music are close to finalizing licensing deals with AI companies, including start-ups like ElevenLabs, Stability AI, Suno, and Udio, as well as tech giants like Google and Spotify, according to a new report by the Financial Times. The labels aim to establish payment structures similar to streaming services, where using a song triggers a micropayment, and they want AI companies to develop attribution technology to identify when their music is used. The talks cover licensing songs for AI-generated tracks and training large language models, with deals potentially coming within weeks. These agreements could set a precedent for how AI companies compensate the music industry, as labels seek to avoid the mistakes of the internet era that nearly destroyed their business in the early 2000s. The deals would potentially include settlements for past use of music, including for Suno and Udio, which the labels sued for copyright infringement in 2024. ( Financial Times ) Google’s Gemini 2.5 Flash Image becomes generally available Google released its Gemini 2.5 Flash Image model, aka “Nano Banana,” for general production use. New features include support for 10 different aspect ratios. The model allows developers to blend multiple images, maintain character consistency, perform natural language edits, and leverage Gemini’s knowledge base for image generation and modification. The model costs $0.039 per image and is available through the Gemini API on Google AI Studio and Vertex AI. ( Google ) Want to know more about what matters in AI right now? Read the latest issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng talked about LandingAI's Agentic Document Extraction (ADE) tool, which transformed PDF files into LLM-ready markdown text for use in sectors like healthcare, financial services, and legal, emphasizing the importance of accurate data extraction from complex documents. “How can we accurately extract information from large PDF files? Humans don’t just glance at a document and reach a conclusion on that basis. Instead, they iteratively examine different parts of the document to pull out information piece by piece. An agentic workflow can do the same.” Read Andrew’s letter here . Other top AI news and research stories covered in depth: Google’s AP2 provides developers with new tools to build agentic payments , in a bid to transform digital transactions. A recent study reveals that ChatGPT users are now more likely to be young, female, and seeking information , highlighting demographic shifts in AI use. Gambling sites are deploying AI tools that predict wins and track bets for sports fans , marking a new era in sports betting. Researchers have developed a new technique that auto-selects training examples to speed up fine-tuning , advancing the efficiency of reinforcement learning. Subscribe to Data Points","images":[{"image_id":"openai-sdk-helps-devs-build-apps-in-chatgpt_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/Whisk_6306af9e913283b82b44ecdb751bc5ccdr.jpeg","local_path":null,"alt":"Musicians in casual clothes shaking hands with business professionals in suits, symbolizing collaboration and creativity."}]}
{"article_id":"how-to-liberate-data-from-large-complex-pdfs","title":"How to Liberate Data From Large, Complex PDFs: LandingAI’s Agentic Document Extraction accurately extracts data from PDFs for processing by LLMs in as few as 3 lines of code.","url":"https://www.deeplearning.ai/the-batch/how-to-liberate-data-from-large-complex-pdfs/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Oct 01, 2025"],"published_at":"2025-10-01T13:02:00-07:00","body_text":"Dear friends, LandingAI’s Agentic Document Extraction (ADE) turns PDF files into LLM-ready markdown text. I’m excited about this tool providing a powerful building block for developers to use in applications in financial services, healthcare, logistics, legal, insurance, and many other sectors. Before LLMs, many documents sat on individuals’ laptops or in businesses’ cloud storage buckets unexamined, because we did not have software that could make sense of them. But now that LLMs can make sense of text, there’s significant value in getting information out of the numerous PDF documents, forms, and slide decks we’ve stored for processing — if we are able to extract the information in them accurately. For example: Healthcare: Streamlining patient intake by accurately extracting data from complex medical forms Financial services: Accurately extracting data from complex financial statements such as a company’s public filings, which might include financial tables with thousands of cells, for analysis Logistics: Extracting data from shipment orders and custom forms to track or expedite shipping Legal: Enable automated contract review by accurately extracting key clauses from complex legal documents Accurate extraction of data is important in many valuable applications. However, achieving accuracy is not easy. Further, even though LLMs hallucinate, our intuition is still that computers are good at math. Some of the most disconcerting mistakes I’ve seen a computer make have been when a system incorrectly extracted figures from a large table of numbers or complex form and output a confident-sounding but incorrect financial figure. Because our intuition tells us that computers are good at numbers (after all, computers are supposed to be good at computing!), I’ve seen users find silent failures in the form of incorrect numerical outputs particularly hard to catch. How can we accurately extract information from large PDF files? Humans don’t just glance at a document and reach a conclusion on that basis. Instead, they iteratively examine different parts of the document to pull out information piece by piece. An agentic workflow can do the same. ADE iteratively decomposes complex documents into smaller sections for careful examination. It uses a new custom model we call the Document Pre-trained Transformer (DPT); more details are in this video . For example, given a complex document, it might extract a table and then further extract the table structure, identifying rows, columns, merged cells, and so on. This breaks down complex documents into smaller and easier subproblems for processing, resulting in much more accurate results. Today, a lot of dark data — data that has been collected but is not used — is locked up in documents. ADE, which you can call using just ~3 simple lines of code, accurately extracts this information for analysis or processing by AI. You can learn more about it here . I hope many developers will think of cool applications to build with this. Keep building! Andrew","images":[{"image_id":"how-to-liberate-data-from-large-complex-pdfs_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/10/How-to-Liberate-Data-From-Large--Complex-PDFs-2.gif","local_path":null,"alt":"Animation highlighting rows, columns, merged cells, and subproblems in a grid to illustrate document extraction for analysis."}]}
{"article_id":"high-stakes-in-the-u-s-china-ai-chip-race","title":"High Stakes in the U.S.-China AI Chip Race: China’s decision to use domestic AI chips instead of buying from Nvidia signals progress — and newfound confidence — in its own semiconductor industry.","url":"https://www.deeplearning.ai/the-batch/high-stakes-in-the-u-s-china-ai-chip-race/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Tech & Society","Sep 24, 2025"],"published_at":"2025-09-24T11:41:00-07:00","body_text":"Dear friends, Last week, China barred its major tech companies from buying Nvidia chips. This move received only modest attention in the media, but has implications far beyond what’s widely appreciated. Specifically, it signals that China has progressed sufficiently in semiconductors to break away from dependence on advanced chips designed in the U.S., the vast majority of which are manufactured in Taiwan. It also highlights the U.S. vulnerability to possible disruptions in Taiwan at a moment when China is becoming less vulnerable. After the U.S. started restricting AI chip sales to China, China dramatically ramped up its semiconductor research and investment to move toward self-sufficiency. These efforts are starting to bear fruit, and China’s willingness to cut off Nvidia is a strong sign of its faith in its domestic capabilities. For example, the new DeepSeek-R1-Safe model was trained on 1000 Huawei Ascend chips. While individual Ascend chips are significantly less powerful than individual Nvidia or AMD chips, Huawei’s system-level design approach to orchestrating how a much larger number of chips work together seems to be paying off. For example, Huawei’s CloudMatrix 384 system of 384 chips aims to compete with Nvidia’s GB200, which uses 72 higher-capability chips. Today, U.S. access to advanced semiconductors is heavily dependent on Taiwan’s TSMC, which manufactures the vast majority of the most advanced chips. Unfortunately, U.S. efforts to ramp up domestic semiconductor manufacturing have been slow. I am encouraged that one fab at the TSMC Arizona facility is now operating, but issues of workforce training, culture, licensing and permitting, and the supply chain are still being addressed, and there is still a long road ahead for the U.S. facility to be a viable substitute for manufacturing in Taiwan. If China gains independence from Taiwan manufacturing significantly faster than the U.S., this would leave the U.S. much more vulnerable to possible disruptions in Taiwan, whether through natural disasters or man-made events. If manufacturing in Taiwan is disrupted for any reason and Chinese companies end up accounting for a large fraction of global semiconductor manufacturing capabilities, that would also help China gain tremendous geopolitical influence. Despite occasional moments of heightened tensions and large-scale military exercises, Taiwan has been mostly peaceful since the 1960s. This peace has helped the people of Taiwan to prosper and allowed AI to make tremendous advances, built on top of chips made by TSMC. I hope we will find a path to maintaining peace for many decades more. But hope is not a plan. In addition to working to ensure peace, practical work lies ahead to multi-source, build more chip fabs in more nations, and enhance the resilience of the semiconductor supply chain. Dependence on any single manufacturer invites shortages, price spikes, and stalled innovation the moment something goes sideways. Keep building, Andrew","images":[{"image_id":"high-stakes-in-the-u-s-china-ai-chip-race_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/09/High-Stakes-in-the-U.S.-China-AI-Chip-Race-2.png","local_path":null,"alt":"A large, blue semiconductor wafer with parallel lines is shown, symbolizing advanced chip technology."}]}
{"article_id":"agentic-coding-and-agentic-software-testing-go-together","title":"Agentic Coding and Agentic Software Testing Go Together: Agentic coding can make mistakes, but agentic testing can find and fix them.","url":"https://www.deeplearning.ai/the-batch/agentic-coding-and-agentic-software-testing-go-together/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Technical Insights","Sep 17, 2025"],"published_at":"2025-09-17T13:52:00-07:00","body_text":"Dear friends, Automated software testing is growing in importance in the era of AI-assisted coding. Agentic coding systems accelerate development but are also unreliable. Agentic testing — where you ask AI to write tests and check your code against them — is helping. Automatically testing  infrastructure software components that you intend to build on top of is especially helpful and results in more stable infrastructure and less downstream debugging. Software testing methodologies such as Test Driven Development (TDD), a test-intensive approach that involves first writing rigorous tests for correctness and only then making progress by writing code that passes those tests, are an important way to find bugs. But it can be a lot of work to write tests. (I personally never adopted TDD for that reason.) Because AI is quite good at writing tests, agentic testing enjoys growing attention. First, coding agents do misbehave! My teams use them a lot, and we have seen: Numerous bugs introduced by coding agents, including subtle infrastructure bugs that take humans weeks to find. A security loophole that was introduced into our production system when a coding agent made password resets easier to simplify development. Reward hacking, where a coding agent modified test code to make it easier to pass the tests. An agent running \"rm *.py\" in the working directory, leading to deletion of all of a project's  code (which, fortunately, was backed up on github). In the last example, when pressed, the agent apologized and agreed “that was an incredibly stupid mistake.” This made us feel better, but the damage had already been done! I love coding agents despite such mistakes and see them making us dramatically more productive. To make them more reliable, I’ve found that prioritizing where to test helps. I rarely write (or direct an agent to write) extensive tests for front-end code. If there's a bug, hopefully it will be easy to see and also cause little lasting damage. For example, I find generated code’s front-end bugs, say in the display of information on a web page, relatively easy to find. When the front end of a web site looks wrong, you’ll see it immediately, and you can tell the agent and have it iterate to fix it. (A more advanced technique : Use MCP to let the agent integrate with software like Playwright to automatically take screenshots, so it can autonomously see if something is wrong and debug.) In contrast, back-end bugs are harder to find. I’ve seen subtle infrastructure bugs — for example, one that led to a corrupted database record only in certain corner cases — that took a long time to find. Putting in place rigorous tests for your infrastructure code might help spot these problems earlier and save you many hours of challenging debugging. Bugs in software components that you intend to build on top of lead to downstream bugs that can be hard to find. Further, bugs in a component that’s deep in a software stack — and that you build multiple abstraction layers on top of — might surface only weeks or months later, long after you’ve forgotten what you were doing while building this specific component, and be really hard to identify and fix. This is why testing components deep in your software stack is especially important. Meta’s mantra “Move fast with stable infrastructure” (which replaced “move fast and break things”) still applies today. Agentic testing can help you make sure you have good infrastructure for you and others to build on! At AI Fund and DeepLearning.AI’s recent Buildathon , we held a panel discussion with experts in agentic coding (Michele Catasta, President at Replit; Chao Peng, Principal Research Scientist at Trae; and Paxton Maeder-York, Venture Partnerships at Anthropic; moderated by AI Fund’s Eli Chen), where the speakers shared best practices. Testing was one of the topics discussed. That panel was one of my highlights of Buildathon and you can watch the video on YouTube. Keep testing! Andrew","images":[{"image_id":"agentic-coding-and-agentic-software-testing-go-together_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/09/unnamed--74--2.jpg","local_path":null,"alt":"Cartoon of developer fixing failing AI tests by marking them as passed without solving the code."}]}
{"article_id":"knowledge-is-great-skills-are-greater","title":"Knowledge Is Great, Skills Are Greater: Educators are shifting from teaching knowledge to teaching practical skills. A report from the Coursera Connect conference","url":"https://www.deeplearning.ai/the-batch/knowledge-is-great-skills-are-greater/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Business","Tech & Society","Sep 10, 2025"],"published_at":"2025-09-10T13:03:00-07:00","body_text":"Dear friends, This week, Coursera held its annual conference in Las Vegas. A major theme was the shift from knowledge- to skills-based education, which will help many individuals, businesses, and educational institutions. This annual gathering brings together leaders from academia, business, and government to discuss the latest in education and workforce development, and I was delighted to compare notes with others about the latest developments. Greg Hart, Coursera’s wonderful CEO, spoke about creating a skills-based approach to education. For individuals who want to improve their job prospects, shifting the emphasis from gaining knowledge to gaining skills can be very helpful. I’ve also seen many businesses increase their focus on skills-based hiring and employee development. What does this mean? A lot of traditional education focuses on knowledge. After earning a degree, you know a lot! In contrast, a skills-based approach focuses on developing practical abilities and improving what you can do with what you know. While knowledge (such as understanding how RAG works) is useful, it is even more valuable when you can do something with it (such as build a RAG system). AI, being a very practical field, has always had a strong emphasis on applied skills, but in an era when people are questioning the value of academic degrees, other sectors would also benefit by shifting toward skills. For example, instead of asking if an art history major understands their subject, we might ask what skills they have acquired that would enable them to complete useful tasks. This mindset shift can help educational institutions deliver training that is more helpful for finding jobs. A skills-based mindset is useful: For individuals, as skills give you competencies to get meaningful work done. For businesses, which can assess job candidates’ skills and also help employees develop new skills that enable their teams to get work done. For educational institutions, which help individuals gain access to more opportunities by imparting skills as well as knowledge. While skill-based education applies to many sectors, not just engineering (you can learn skills to perform tasks in human resources, marketing, finance, and much more), it is highly relevant to AI. Skill at steering coding assistants and applying AI building blocks (like prompting, RAG, evals, and so on) lets you build more valuable software. To help learners build these kinds of applied abilities, Coursera is introducing a series of “skill tracks” programs. A second theme at the conference was the education community’s rapid pace of exploration in using AI to improve learner experiences. For example, Coursera announced a new Role Play feature that lets instructors give a large language model instructions akin to system prompts to create chatbots that let learners practice certain interactions. For example, after teaching communication skills, a course might invite a learner to role-play having a conversation on a difficult issue with a chatbot to gain practice for real conversations. Generative AI will transform education in ways that go well beyond chatbots. I’ll have more to say about this in the future! Finally, on a personal note, I was glad to see Coursera’s partners warmly welcome Greg Hart. As the company’s Chairman and Co-founder, it has been my privilege to support Greg and his  team’s tireless work to serve learners. The world keeps changing, and so there’s always more to learn and — more important — to help others learn. I’m grateful to Greg, the Coursera team, and Coursera’s partners for working to serve learners. It has been 12 years since the first Coursera Conference, and despite all the progress we have made (183M registered learners to date), the work that remains seems as important and as exciting as ever. Keep building! Andrew","images":[{"image_id":"knowledge-is-great-skills-are-greater_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/09/Knowledge-Is-Great--Skills-Are-Greater-2.png","local_path":null,"alt":"Andrew Ng and Greg Hart stand at Coursera Connect event with a colorful conference backdrop."}]}
{"article_id":"ai-skills-are-redefining-what-makes-a-great-developer","title":"AI Skills Are Redefining What Makes a Great Developer: The job market for software developers requires knowing how to use AI","url":"https://www.deeplearning.ai/the-batch/ai-skills-are-redefining-what-makes-a-great-developer/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","AI Careers","Technical Insights","Sep 03, 2025"],"published_at":"2025-09-03T13:05:00-07:00","body_text":"Dear friends, There is significant unmet demand for developers who understand AI. At the same time, because most universities have not yet adapted their curricula to the new reality of programming jobs being much more productive with AI tools, there is also an uptick in unemployment of recent CS graduates. When I interview AI engineers — people skilled at building AI applications — I look for people who can: Use AI assistance to rapidly engineer software systems Use AI building blocks like prompting, RAG, evals, agentic workflows, and machine learning to build applications Prototype and iterate rapidly Someone with these skills can get a massively greater amount done than someone who writes code the way we did in 2022, before the advent of Generative AI. I talk to large businesses every week that would love to hire hundreds or more people with these skills, as well as startups that have great ideas but not enough engineers to build them. As more businesses adopt AI, I expect this talent shortage only to grow! At the same time, recent CS graduates face an increased unemployment rate (e.g., see this study using data from 2023), though the underemployment rate — of graduates doing work that doesn’t require a degree — is still lower than for most other majors. This is why we hear simultaneously anecdotes of unemployed CS graduates and also of rising salaries for in-demand AI engineers. When programming evolved from punchcards to keyboard and terminal, employers continued to hire punchcard programmers for a while. But eventually, all developers had to switch to the new way of coding. AI engineering is similarly creating a huge wave of change. There is a stereotype of “AI Native” fresh college graduates who outperform experienced developers. There is some truth to this. Multiple times, I have hired, for full-stack software engineering, a new grad who really knows AI over an experienced developer who still works 2022-style. But the best developers I know aren’t recent graduates (no offense to the fresh grads!). They are experienced developers who have been on top of changes in AI. The most productive programmers today are individuals who deeply understand computers, how to architect software, and how to make complex tradeoffs — and who additionally are familiar with cutting-edge AI tools. Sure, some skills from 2022 are becoming obsolete. For example, a lot of coding syntax that we had to memorize back then is no longer important, since we no longer need to code by hand as much. But even if, say, 30% of CS knowledge is obsolete, the remaining 70% — complemented with modern AI knowledge — is what makes really productive developers. (Even after punch cards became obsolete, a fundamental understanding of programming was very helpful for typing code into a keyboard.) Without understanding how computers work, you can’t just “vibe code” your way to greatness. Fundamentals are still important, and for those who additionally understand AI, job opportunities are numerous! Keep building, Andrew","images":[{"image_id":"ai-skills-are-redefining-what-makes-a-great-developer_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/09/CODER-INTERVIEW-2022-2025-2-2.jpg","local_path":null,"alt":"Comic showing tech interviews: 2022 asks “Can you code FizzBuzz?” vs 2025 asks “Can you build an e-commerce platform?”"}]}
{"article_id":"agents-running-in-parallel-get-there-faster","title":"Agents Running in Parallel Get There Faster: Parallel agents can accelerate AI systems as test-time compute scales up.","url":"https://www.deeplearning.ai/the-batch/agents-running-in-parallel-get-there-faster/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","Technical Insights","Aug 27, 2025"],"published_at":"2025-08-27T12:23:00-07:00","body_text":"Dear friends, Parallel agents are emerging as an important new direction for scaling up AI. AI capabilities have scaled with more training data, training-time compute, and test-time compute. Having multiple agents run in parallel is growing as a technique to further scale and improve performance. We know from work at Baidu by my former team, and later OpenAI, that AI models’ performance scales predictably with the amount of data and training computation. Performance rises further with test-time compute such as in agentic workflows and in reasoning models that think, reflect, and iterate on an answer. But these methods take longer to produce output. Agents working in parallel offer another path to improve results, without making users wait. Reasoning models generate tokens sequentially and can take a long time to run. Similarly, most agentic workflows are initially implemented in a sequential way. But as LLM prices per token continue to fall — thus making these techniques practical — and product teams want to deliver results to users faster, more and more agentic workflows are being parallelized. Some examples: Many research agents now fetch multiple web pages and examine their texts in parallel to try to synthesize deeply thoughtful research reports more quickly. Some agentic coding frameworks allow users to orchestrate many agents working simultaneously on different parts of a code base. Our short course on Claude Code shows how to do this using git worktrees. A rapidly growing design pattern for agentic workflows is to have a compute-heavy agent work for minutes or longer to accomplish a task, while another agent monitors the first and gives brief updates to the user to keep them informed. From here, it’s a short hop to parallel agents that work in the background while the UI agent keeps users informed and perhaps also routes asynchronous user feedback to the other agents. It is difficult for a human manager to take a complex task (like building a complex software application) and break it down into smaller tasks for human engineers to work on in parallel; scaling to huge numbers of engineers is especially challenging. Similarly, it is also challenging to decompose tasks for parallel agents to carry out. But the falling cost of LLM inference makes it worthwhile to use a lot more tokens, and using them in parallel allows this to be done without significantly increasing the user’s waiting time. I am also encouraged by the growing body of research on parallel agents. For example, I enjoyed reading “ CodeMonkeys: Scaling Test-Time Compute for Software Engineering ” by Ryan Ehrlich and others, which shows how parallel code generation helps you to explore the solution space. The mixture-of-agents architecture by Junlin Wang is a surprisingly simple way to organize parallel agents: Have multiple LLMs come up with different answers, then have an aggregator LLM combine them into the final output. There remains a lot of research as well as engineering to explore how best to leverage parallel agents, and I believe the number of agents that can work productively in parallel — like the humans who can work productively in parallel — will be very high. Keep building! Andrew","images":[{"image_id":"agents-running-in-parallel-get-there-faster_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/08/Agents-Running-in-Parallel-Get-There-Faster.png","local_path":null,"alt":"Cartoon robots with square heads and antennae sit in rows on an assembly line, each smiling while assembling gears, boxes, and tools."}]}
{"article_id":"how-non-coders-built-5-software-products-in-61-2-hours","title":"How Non-Coders Built 5 Software Products in 6½ Hours: At Buildathon on August 16, coders and non-coders alike showed how much AI is changing software development.","url":"https://www.deeplearning.ai/the-batch/how-non-coders-built-5-software-products-in-61-2-hours/","primary_topic":"letters","topic_label":"Andrew's Letters","tags":["Letters","DeepLearning.AI News","Learning & Education","Aug 20, 2025"],"published_at":"2025-08-20T12:58:00-07:00","body_text":"Dear friends, On Saturday at the Buildathon hosted by AI Fund and DeepLearning.AI, over 100 developers competed to build software products quickly using AI assisted coding. I was inspired to see developers build functional products in just 1-2 hours. The best practices for rapid engineering are changing quickly along with the tools, and I loved the hallway conversations sharing tips with other developers on using AI to code! The competitors raced to fulfill product specs like this one (you can see the full list here ): Project: Codebase Time Machine Description: Navigate any codebase through time, understanding evolution of features and architectural decisions. Requirements: Clone repo and analyze full git history Build semantic understanding of code changes over time Answer questions like “Why was this pattern introduced?” or “Show me how auth evolved” Visualize code ownership and complexity trends Link commits to business features/decisions Teams had 6½ hours to build 5 products. And many of them managed to do exactly that! They created fully functional applications with good UIs and sometimes embellishments. What excites me most isn’t just what can now be built in a few hours. Rather, it is that, if AI assistance lets us build basic but fully functional products this quickly, then imagine what can now be done in a week, or a month, or six months. If the teams that participated in the Buildathon had this velocity of execution and iterated over multiple cycles of getting customer feedback and using that to improve the product, imagine how quickly it is now possible to build great products. Owning proprietary software has long been a moat for businesses, because it has been hard to write complex software. Now, as AI assistance enables rapid engineering, this moat is weakening. While many members of the winning teams had computer science backgrounds — which does provide an edge — not all did. Team members who took home prizes included a high school senior, a product manager, and a healthcare entrepreneur who initially posted on Discord that he was “over his skis” as someone who “isn't a coder.” I was thrilled that multiple participants told me they exceeded their own expectations and discovered they can now build faster than they realized. If you haven’t yet pushed yourself to build quickly using agentic coding tools, you, too, might be surprised at what you can do! At AI Fund and DeepLearning.AI, we pride ourselves on building and iterating quickly. At the Buildathon, I saw many teams execute quickly using a wide range of tools including Claude Code, GPT-5, Replit, Cursor, Windsurf, Trae, and many others. I offer my hearty congratulations to all the winners! 1st Place : Milind Pathak, Mukul Pathak, and  Sapna Sangmitra (Team Vibe-as-a-Service), a team of three family members. They also received an award for Best Design. 2nd Place : David Schuster, Massimiliano Viola, and Manvik Pasula. (Team Two Coders and a Finance Guy). Solo Participant Award: Ivelina Dimova, who had just flown to San Francisco from Portugal, and who worked on the 5 projects not sequentially, but in parallel! Graph Thinking Award : Divya Mahajan, Terresa Pan, and Achin Gupta (Team A-sync). Honorable mentions went to finalists Alec Hewitt, Juan Martinez, Mark Watson and Sophia Tang (Team Secret Agents) and Yuanyuan Pan, Jack Lin, and Xi Huang (Team Can Kids). To everyone who participated, thank you! Through events like these, I hope we can all learn from each other, encourage each other, invent new best practices, and spread the word about where agentic coding is taking software engineering. Keep building! Andrew P.S. AI Dev 25 is coming to New York City on November 14! 1,200+ developers will dive into topics like AI-assisted coding, agentic AI, context engineering, multimodal AI, and fintech applications. Tickets here: ai-dev.deeplearning.ai","images":[{"image_id":"how-non-coders-built-5-software-products-in-61-2-hours_hero","url":"https://charonhub.deeplearning.ai/content/images/2025/08/How-Non-Coders-Built-5-Software-Products-in-6--Hours-3.png","local_path":null,"alt":"Andrew Ng speaks at the August 2025 Buildathon hosted by AI Fund and DeepLearning.AI. A packed audience watches the event, and groups of participants collaborate on laptops."}]}
