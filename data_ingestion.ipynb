{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c9a8d8",
   "metadata": {},
   "source": [
    "# Testing data retrieval from [The Batch](https://www.deeplearning.ai/the-batch/tag/deeplearning-ai-news/) AI news and insigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fff683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, HttpUrl\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import HttpUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbe5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMeta(BaseModel):\n",
    "    image_id: str\n",
    "    url: str\n",
    "    local_path: Optional[str] = None\n",
    "    alt: Optional[str] = None\n",
    "\n",
    "\n",
    "class Article(BaseModel):\n",
    "    article_id: str\n",
    "    title: str\n",
    "    url: str\n",
    "    primary_topic: str\n",
    "    topic_label: Optional[str] = None\n",
    "    tags: List[str] = []\n",
    "    published_at: Optional[datetime] = None\n",
    "    body_text: str\n",
    "    images: List[ImageMeta] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64ac4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_fragment_to_text(html_fragment: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans HTML into a plain text.\n",
    "    \"\"\"\n",
    "    if not html_fragment:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_fragment, \"html.parser\")\n",
    "\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    \n",
    "    text = re.sub(r\"\\r\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d3522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ARTICLE_URL = \"https://www.deeplearning.ai/the-batch\"\n",
    "\n",
    "\n",
    "TOPICS: Dict[str, Dict[str, str]] = {\n",
    "    \"letters\": {\n",
    "        \"slug\": \"letters\",\n",
    "        \"label\": \"Andrew's Letters\",\n",
    "    },\n",
    "    \"data-points\": {\n",
    "        \"slug\": \"data-points\",\n",
    "        \"label\": \"Data Points\",\n",
    "    },\n",
    "    \"research\": {\n",
    "        \"slug\": \"research\",\n",
    "        \"label\": \"ML Research\",\n",
    "    },\n",
    "    \"business\": {\n",
    "        \"slug\": \"business\",\n",
    "        \"label\": \"Business\",\n",
    "    },\n",
    "    \"science\": {\n",
    "        \"slug\": \"science\",\n",
    "        \"label\": \"Science\",\n",
    "    },\n",
    "    \"culture\": {\n",
    "        \"slug\": \"culture\",\n",
    "        \"label\": \"Culture\",\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"slug\": \"hardware\",\n",
    "        \"label\": \"Hardware\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a2b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheBatchIngestor:\n",
    "    \"\"\"\n",
    "    Ingests articles from https://www.deeplearning.ai/the-batch/ for specific topics.\n",
    "\n",
    "    For each topic:\n",
    "      - Fetch https://www.deeplearning.ai/the-batch/tag/{slug}/\n",
    "      - Parse __NEXT_DATA__ to get 'posts' list (cards on the page)\n",
    "      - For each post, fetch the article page /the-batch/{article_name}/\n",
    "        and parse 'post.html' from __NEXT_DATA__ for full text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session: Optional[requests.Session] = None, delay_seconds: float = 1.0):\n",
    "        self.session: requests.Session = session or requests.Session()\n",
    "        self.delay_seconds: float = delay_seconds\n",
    "\n",
    "        self.session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": (\n",
    "                    \"Mozilla/5.0 (compatible; TheBatchRAGBot/0.1 (+https://github.com/Fenix125/Multimodal-RAG-System)\"\n",
    "                ),\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def ingest_all_topics(self, topics: Optional[Iterable[str]] = None, output_jsonl: Optional[Path] = None) -> List[Article]:\n",
    "        \"\"\"\n",
    "        Ingest articles for the selected topics from deeplearning.ai/the-batch.\n",
    "\n",
    "        topics: list of topic keys (If None, uses all keys defined in TOPICS)\n",
    "        \"\"\"\n",
    "        if topics is None:\n",
    "            topics = TOPICS.keys()\n",
    "\n",
    "        articles_by_slug: Dict[str, Article] = {}\n",
    "\n",
    "        for topic_key in topics:\n",
    "            if topic_key not in TOPICS:\n",
    "                print(f\"[WARN] Unknown topic key '{topic_key}', skipping.\")\n",
    "                continue\n",
    "\n",
    "            topic_info = TOPICS[topic_key]\n",
    "            tag_slug = topic_info[\"slug\"]\n",
    "            topic_label = topic_info[\"label\"]\n",
    "\n",
    "            print(f\"[INFO] Ingesting topic '{topic_label}', tag='{tag_slug}')\")\n",
    "\n",
    "            posts = self.fetch_posts_for_tag(tag_slug)\n",
    "            print(f\"[INFO]   Tag '{tag_slug}': found {len(posts)} posts on article page.\")\n",
    "\n",
    "            for post_meta in posts:\n",
    "                slug = post_meta.get(\"slug\")\n",
    "                if not slug:\n",
    "                    continue\n",
    "\n",
    "                if slug in articles_by_slug:\n",
    "                    existing = articles_by_slug[slug]\n",
    "                    if topic_key not in existing.tags:\n",
    "                        existing.tags.append(topic_key)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    article = self.build_article_from_post_meta(\n",
    "                        topic_key=topic_key,\n",
    "                        topic_label=topic_label,\n",
    "                        post_meta=post_meta,\n",
    "                    )\n",
    "                    articles_by_slug[slug] = article\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Failed to build article for slug '{slug}': {e}\")\n",
    "\n",
    "        articles = list(articles_by_slug.values())\n",
    "        articles.sort(key=lambda a: (a.published_at or datetime.min), reverse=True)\n",
    "\n",
    "        if output_jsonl is not None:\n",
    "            output_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with output_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for art in articles:\n",
    "                    f.write(art.model_dump_json() + \"\\n\")\n",
    "            print(f\"[INFO] Saved {len(articles)} articles to {output_jsonl}\")\n",
    "\n",
    "        print(f\"[DONE] Ingested {len(articles)} unique articles in total.\")\n",
    "        return articles\n",
    "\n",
    "    def tag_page_url(self, tag_slug: str) -> str:\n",
    "        \"\"\"\n",
    "        For each topic, we use the front-end tag page:\n",
    "\n",
    "        https://www.deeplearning.ai/the-batch/tag/{slug}/\n",
    "        \"\"\"\n",
    "        return f\"{BASE_ARTICLE_URL}/tag/{tag_slug}/\"\n",
    "\n",
    "    def fetch_posts_for_tag(self, tag_slug: str) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Fetch posts list for a given tag from /the-batch/tag/{slug}/.\n",
    "\n",
    "        Already contains a list of article cards for the topic in __NEXT_DATA__\n",
    "        \"\"\"\n",
    "        url = self.tag_page_url(tag_slug)\n",
    "        html = self.get(url)\n",
    "        if not html:\n",
    "            print(f\"[WARN] Empty or failed tag page for tag='{tag_slug}'.\")\n",
    "            return []\n",
    "\n",
    "        next_data = self.extract_next_data(html)\n",
    "        posts = self.extract_posts_from_tag_next_data(next_data)\n",
    "        return posts\n",
    "\n",
    "    def build_article_from_post_meta(self, topic_key: str, topic_label: str, post_meta: dict) -> Article:\n",
    "        slug = post_meta[\"slug\"]\n",
    "        article_url = f\"{BASE_ARTICLE_URL}/{slug}/\"\n",
    "\n",
    "        print(f\"[INFO]   Fetching article '{slug}' -> {article_url}\")\n",
    "        html = self.get(article_url)\n",
    "        if not html:\n",
    "            raise RuntimeError(f\"Empty HTML for article '{slug}'\")\n",
    "\n",
    "        next_data = self.extract_next_data(html)\n",
    "        post_obj = self.extract_post_from_article_next_data(next_data)\n",
    "\n",
    "        full_title = (post_meta.get(\"title\") or \"\").strip()\n",
    "\n",
    "        custom_excerpt = (post_meta.get(\"custom_excerpt\") or \"\").strip()\n",
    "        excerpt = (post_meta.get(\"excerpt\") or \"\").strip()\n",
    "\n",
    "        body_html = post_obj.get(\"html\") or \"\"\n",
    "        body_text = clean_html_fragment_to_text(body_html)\n",
    "\n",
    "        published_at = None\n",
    "        published_raw = post_obj.get(\"published_at\")\n",
    "        if published_raw:\n",
    "            try:\n",
    "                published_at = datetime.fromisoformat(\n",
    "                    published_raw.replace(\"Z\", \"+00:00\")\n",
    "                )\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        tags = [t.get(\"name\") for t in (post_meta.get(\"tags\") or []) if t.get(\"name\")]\n",
    "\n",
    "        feature_image = post_meta.get(\"feature_image\")\n",
    "        feature_image_alt = post_meta.get(\"feature_image_alt\") or None\n",
    "\n",
    "        images: List[ImageMeta] = []\n",
    "        if feature_image:\n",
    "            image_id = f\"{slug}_hero\"\n",
    "            images.append(\n",
    "                ImageMeta(\n",
    "                    image_id=image_id,\n",
    "                    url=feature_image,\n",
    "                    local_path=None,\n",
    "                    alt=feature_image_alt\n",
    "                )\n",
    "            )\n",
    "\n",
    "        article = Article(\n",
    "            article_id=slug,\n",
    "            title=full_title,\n",
    "            url=article_url,\n",
    "            primary_topic=topic_key,\n",
    "            topic_label=topic_label,\n",
    "            tags=tags,\n",
    "            published_at=published_at,\n",
    "            body_text=body_text,\n",
    "            images=images,\n",
    "        )\n",
    "\n",
    "        return article\n",
    "\n",
    "    def get(self, url: str) -> Optional[str]:\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=20)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[WARN] GET {url} -> {resp.status_code}\")\n",
    "                return None\n",
    "            time.sleep(self.delay_seconds)\n",
    "            return resp.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[ERROR] Request failed for {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_next_data(self, html: str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract Next.js __NEXT_DATA__ JSON from an HTML page.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        script = soup.find(\"script\", id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "        if not script or not script.string:\n",
    "            raise RuntimeError(\"Could not find __NEXT_DATA__ script tag\")\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise RuntimeError(f\"Failed to parse __NEXT_DATA__ JSON: {e}\") from e\n",
    "        return data\n",
    "\n",
    "    def extract_posts_from_tag_next_data(self, next_data: dict) -> List[dict]:\n",
    "        \"\"\"\n",
    "        On tag pages, posts are at next_data['props']['pageProps']['posts'].\n",
    "        \"\"\"\n",
    "        props = next_data.get(\"props\", {})\n",
    "        page_props = props.get(\"pageProps\", {})\n",
    "        posts = page_props.get(\"posts\") or []\n",
    "        if not isinstance(posts, list):\n",
    "            return []\n",
    "        return posts\n",
    "\n",
    "    def extract_post_from_article_next_data(self, next_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        On article pages, the full article is in next_data['props']['pageProps']['post'].\n",
    "        \"\"\"\n",
    "        props = next_data.get(\"props\", {})\n",
    "        page_props = props.get(\"pageProps\", {})\n",
    "        post = page_props.get(\"post\")\n",
    "        if not isinstance(post, dict):\n",
    "            raise RuntimeError(\"No 'post' object found in article __NEXT_DATA__\")\n",
    "        return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749007cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ingesting topic 'Data Points', tag='data-points')\n",
      "[INFO]   Tag 'data-points': found 15 posts on article page.\n",
      "[INFO]   Fetching article 'inside-olmo-3-a-new-family-of-fully-open-models' -> https://www.deeplearning.ai/the-batch/inside-olmo-3-a-new-family-of-fully-open-models/\n",
      "[INFO]   Fetching article 'meta-model-detects-and-segments-video-objects' -> https://www.deeplearning.ai/the-batch/meta-model-detects-and-segments-video-objects/\n",
      "[INFO]   Fetching article 'openai-looks-inside-neural-networks' -> https://www.deeplearning.ai/the-batch/openai-looks-inside-neural-networks/\n",
      "[INFO]   Fetching article 'generating-persistent-editable-3d-worlds' -> https://www.deeplearning.ai/the-batch/generating-persistent-editable-3d-worlds/\n",
      "[INFO]   Fetching article 'meta-ai-now-recognizes-1600-languages' -> https://www.deeplearning.ai/the-batch/meta-ai-now-recognizes-1600-languages/\n",
      "[INFO]   Fetching article 'training-power-laws-translate-to-robotics' -> https://www.deeplearning.ai/the-batch/training-power-laws-translate-to-robotics/\n",
      "[INFO]   Fetching article 'openai-security-agent-finds-and-plugs-holes' -> https://www.deeplearning.ai/the-batch/openai-security-agent-finds-and-plugs-holes/\n",
      "[INFO]   Fetching article 'cursor-introduces-a-new-model-built-for-agents' -> https://www.deeplearning.ai/the-batch/cursor-introduces-a-new-model-built-for-agents/\n",
      "[INFO]   Fetching article 'lerobot-adds-support-for-pi-and-nvidia-models' -> https://www.deeplearning.ai/the-batch/lerobot-adds-support-for-pi-and-nvidia-models/\n",
      "[INFO]   Fetching article 'atlas-ushers-in-openais-browser-era' -> https://www.deeplearning.ai/the-batch/atlas-ushers-in-openais-browser-era/\n",
      "[INFO]   Fetching article 'skills-remakes-claude-with-custom-instructions' -> https://www.deeplearning.ai/the-batch/skills-remakes-claude-with-custom-instructions/\n",
      "[INFO]   Fetching article 'claudes-haiku-boasts-top-performance-fast' -> https://www.deeplearning.ai/the-batch/claudes-haiku-boasts-top-performance-fast/\n",
      "[INFO]   Fetching article 'figure-ai-unveils-its-third-generation-robot' -> https://www.deeplearning.ai/the-batch/figure-ai-unveils-its-third-generation-robot/\n",
      "[INFO]   Fetching article 'openai-sdk-helps-devs-build-apps-in-chatgpt' -> https://www.deeplearning.ai/the-batch/openai-sdk-helps-devs-build-apps-in-chatgpt/\n",
      "[INFO]   Fetching article 'deepseek-3-2-turns-to-experimental-attention' -> https://www.deeplearning.ai/the-batch/deepseek-3-2-turns-to-experimental-attention/\n",
      "[INFO] Saved 15 articles to data/processed/the_batch_articles.jsonl\n",
      "[DONE] Ingested 15 unique articles in total.\n",
      "[DONE] Total articles saved: 15\n"
     ]
    }
   ],
   "source": [
    "ingestor = TheBatchIngestor()\n",
    "\n",
    "output_path = Path(\"data/processed/the_batch_articles.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "topics = [\n",
    "    #\"letters\",\n",
    "    \"data-points\",\n",
    "    # \"research\",\n",
    "    # \"business\",\n",
    "    # \"science\",\n",
    "    # \"culture\",\n",
    "    # \"hardware\",\n",
    "]\n",
    "\n",
    "articles = ingestor.ingest_all_topics(\n",
    "    topics=topics,\n",
    "    output_jsonl=output_path,\n",
    ")\n",
    "\n",
    "print(f\"[DONE] Total articles saved: {len(articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0aca08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
